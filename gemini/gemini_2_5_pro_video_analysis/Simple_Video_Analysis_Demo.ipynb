{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHJxUTN50Zo"
      },
      "source": [
        "## Gemini 2.5 Pro: Simple Video Analysis Demo (Files for Video & Reference)\n",
        "\n",
        "This notebook demonstrates how to use the Gemini API (`gemini-2.5-pro-exp-03-25`) to analyze a video with a reference text file. It’s designed to be simple and clean, focusing on uploading files, processing them, generating an analysis, and showing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAcBhxnm50Zp",
        "outputId": "3083cee6-2c8d-4167-ca5c-08dee5994c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: gemini-2.5-pro-exp-03-25\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup: Import Libraries & Configure API Key\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Configure API Key (replace 'GOOGLE_API_KEY' with your key name in Colab secrets)\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Define the model\n",
        "MODEL_NAME = 'gemini-2.5-pro-exp-03-25'\n",
        "print(f\"Using model: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V2cIhF450Zq",
        "outputId": "f8cfa941-4f87-42c0-b114-4c6446108966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading file: video.mp4...\n",
            "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/99zfw6z8tkhr\n",
            "Uploading file: reference.txt...\n",
            "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/1j6vm4i8dgpn\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Upload Files: Video and Reference Text\n",
        "def upload_file(client, file_name):\n",
        "    print(f\"Uploading file: {file_name}...\")\n",
        "    try:\n",
        "        file_obj = client.files.upload(file=file_name)\n",
        "        print(f\"Completed upload: {file_obj.uri}\")\n",
        "        return file_obj\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_name}' not found. Please upload it to Colab.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file: {e}\")\n",
        "        return None\n",
        "\n",
        "# File names (change these to your actual files)\n",
        "video_file_name = 'video.mp4'\n",
        "reference_file_name = 'reference.txt'\n",
        "\n",
        "# Upload both files\n",
        "video_file = upload_file(client, video_file_name)\n",
        "ref_file = upload_file(client, reference_file_name)\n",
        "\n",
        "# Ensure both files are uploaded\n",
        "if not video_file or not ref_file:\n",
        "    raise ValueError(\"One or both files failed to upload. Please check and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4K77-9D50Zq",
        "outputId": "af9c16bc-0388-42a9-9ee0-2a1ca93e7ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files/99zfw6z8tkhr.. Done. State: ACTIVE\n",
            "Processing files/1j6vm4i8dgpn Done. State: ACTIVE\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Wait for Files to Process\n",
        "def wait_for_file(client, file_obj):\n",
        "    if not file_obj:\n",
        "        return\n",
        "    print(f\"Processing {file_obj.name}\", end='')\n",
        "    while file_obj.state.name == \"PROCESSING\":\n",
        "        print('.', end='', flush=True)\n",
        "        time.sleep(5)  # Check every 5 seconds\n",
        "        file_obj = client.files.get(name=file_obj.name)\n",
        "    print(f\" Done. State: {file_obj.state.name}\")\n",
        "    if file_obj.state.name == \"FAILED\":\n",
        "        raise ValueError(f\"File processing failed for {file_obj.name}\")\n",
        "\n",
        "# Wait for both files\n",
        "wait_for_file(client, video_file)\n",
        "wait_for_file(client, ref_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM9kKbJ950Zq"
      },
      "outputs": [],
      "source": [
        "# @title 4. Define the Analysis Prompt\n",
        "prompt = \"\"\"\n",
        "**Act As:** Expert Video Content Analyst & Editor Assistant\n",
        "\n",
        "**Goal:** Analyze the draft video to improve cutting efficiency by:\n",
        "1. Understanding the video’s content and flow.\n",
        "2. Identifying key segments with clear delivery or important topics, using the reference text file as a context guide.\n",
        "3. Providing cleaned transcripts (no \"ums\" or \"ahs\").\n",
        "\n",
        "**Input Sources:**\n",
        "- **Video File:** The draft video (uploaded via API).\n",
        "- **Reference Text File:** Supplementary context (uploaded via API). Prioritize video content over this.\n",
        "\n",
        "**Instructions:**\n",
        "- Watch the entire video to understand its flow and key points.\n",
        "- Pick segments based on video quality and relevance, using the reference text only to validate topics.\n",
        "- Clean transcripts by removing filler words without altering meaning.\n",
        "\n",
        "**Output Format (Markdown):**\n",
        "```markdown\n",
        "## Video Analysis for Efficient Cutting\n",
        "\n",
        "**Cut Segment 1**\n",
        "* **Timestamp:** [MM:SS - MM:SS]\n",
        "* **Summary:** [Why this segment stands out.]\n",
        "* **Transcript:** [Cleaned text.]\n",
        "\n",
        "**Cut Segment 2**\n",
        "* **Timestamp:** [MM:SS - MM:SS]\n",
        "* **Summary:** [Why this segment stands out.]\n",
        "* **Transcript:** [Cleaned text.]\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRF2J3cz50Zr",
        "outputId": "ad7eb475-6d29-4917-e0f2-7a949f9079cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating analysis (this may take a moment)...\n",
            "Analysis complete.\n"
          ]
        }
      ],
      "source": [
        "# @title 5. Generate Analysis\n",
        "print(\"Generating analysis (this may take a moment)...\")\n",
        "try:\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_NAME,\n",
        "        contents=[ref_file, video_file, prompt])\n",
        "    print(\"Analysis complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating analysis: {e}\")\n",
        "    response = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "fiIkihnh50Zr",
        "outputId": "28c56094-0881-4786-8ae9-86a55c210526"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```markdown\n## Video Analysis for Efficient Cutting\n\n**Cut Segment 1**\n* **Timestamp:** [0:06 - 1:32]\n* **Summary:** Introduces the Gemini 2.5 Pro Experimental model, highlighting its status as Google's best model according to the Chatbot Arena Leaderboard, surpassing competitors like GPT-4o and Grok. Mentions the video will cover using it in AI Studio and via API in a notebook.\n* **Transcript:** Hey guys. Hello. Welcome to my channel. And today's topic is about the Gemini 2.5 model, which is released by Google recently, a few days ago. And it turns out to be the most or the best model so far, according to the large language models. And I think today I'm going to walk you through how to use that in the Google AI Studio and also how to call the APIs in a, you know, Python notebook. And if we go to this page, the arena, the arena, like the model performance rankings website, you can see that the Gemini 2.5 Pro is ranked the first, and it's doing the best among the competitors like GPT, Grok, and, you know, Deepseek, other, you know, fancy models. Okay, let's try out the examples.\n\n**Cut Segment 2**\n* **Timestamp:** [1:36 - 2:56]\n* **Summary:** Shows the Google AI Studio interface for Gemini 2.5 Pro Experimental (03-25 version). Points out the model details including the token count (~1M), confirms it's currently free, highlights the knowledge cutoff (Jan 2025), and crucially contrasts the AI Studio rate limit (5 RPM) with the free tier API rate limit (2 RPM / 50 per day).\n* **Transcript:** Okay, here's the Google AI Studio. And if we look at the right, you can see the model is the 2.5. And we can check the input and output pricing. Currently, it's free to use. And you can use for whatever cases, the coding, reasoning, whatever cases. And you can check out the knowledge cut-off is January 2025. And the important thing is that the rate limits. It has five, five requests per minute. For, I think it's for this Google AI Studio inside of that. And for the second one, the two reads per minute, 50 per day, I think that's maybe for the API requests. Right.\n\n**Cut Segment 3**\n* **Timestamp:** [2:58 - 3:45]\n* **Summary:** Demonstrates selecting the \"YouTube video\" input option in Google AI Studio, pasting a URL of a previous video about Gemini fine-tuning, and prompting the model for a full analysis.\n* **Transcript:** So we can try out some fancy functions here. So, I'm really interested in the YouTube videos. So if I just copy paste my channel, my previous video about the practical Gemini Fine-tuning thing... Alright, it can recognize the video and... So let's try something like, Give me a full analysis of this video.\n\n**Cut Segment 4**\n* **Timestamp:** [4:52 - 5:54]\n* **Summary:** Shows the successful analysis of the YouTube video generated by Gemini 2.5 Pro within AI Studio, highlighting its ability to understand the video's content, key concepts (SFT, Vertex AI, JSONL, GCS, metrics), process, target audience, and overall impression. Gives positive feedback to the model.\n* **Transcript:** Okay, here's a full analysis of the video \"Supervised Fine-tuning for Gemini Models\". Main Topic & Goal. The video is a practical tutorial demonstrating how to perform supervised fine-tuning on Google's Gemini language models using the Vertex AI platform within Google Cloud... Key Concepts Explained: Supervised Fine-Tuning (SFT) Briefly explained... Gemini Models: The family of Google models being fine-tuned... Vertex AI: Google Cloud's unified ML platform... JSON Lines (JSONL) Format... Google Cloud Storage (GCS)... Training Metrics: Key indicators like \"Total Loss\"... Process Demonstrated: Introduction & Context... Understanding SFT... Data Preparation... Data Upload... Fine-Tuning Job Setup... Monitoring & Results... Conclusion... Tools & Platforms Used: Google Cloud Platform (GCP)... Vertex AI... Google Cloud Storage (GCS)... Gemini Models: Specifically gemini-2.0-flash-001... Python... Pandas Library... JSON Library... Web Browser... Data: Source: A combination of AI-generated prompts... Initial Format: CSV. Processing: Converted to JSONL... Final Format (JSONL)... Size: A small demo dataset... Target Audience: The video appears targeted towards developers, data scientists, or ML practitioners... Overall Impression: This video is a clear, well-structured, and practical walkthrough... Alright. We got the response from the Gemini 2.5 Pro. And you can see the analysis of this video. And let me check if it's doing correct and if it's really understanding my video contents. Okay, so the main topic... Yeah, it's demonstrating how to perform the supervised fine-tuning. And... I explained the different models of the Gemini for the API fine-tuning. And I explained the JSON formats and how to upload the data to the Cloud Storage, Google Cloud Storage. And checked... also showed how to do my personal project and preparing the data. Yeah, the target... Let's see the overall impression. So the video is clear, well structured, and practical walkthrough... Okay. Yeah, I think it's pretty much capturing all the contents I put in that video. So it's doing really great job. And I have to give a really good response for that. Um, okay.\n\n**Cut Segment 5**\n* **Timestamp:** [9:11 - 9:50]\n* **Summary:** Transitions to the second part of the video, explaining the plan to use the Gemini 2.5 Pro Experimental model via its API within a Colab notebook, specifically demonstrating a practical project. References the API documentation shown earlier.\n* **Transcript:** So I think next, um, what I'm really interested is to use, like, how do we do a personal, like project in Colab and to implement the API, how do we like really use the API requests in the Python notebook. So you can check this documentation for the API.\n\n**Cut Segment 6**\n* **Timestamp:** [11:31 - 12:39]\n* **Summary:** Introduces the Colab notebook designed to rewrite prompts using the Gemini API (specifically `gemini-2.5-pro-exp-03-25` via `genai.Client`). Explains the script's function: read CSV, interact with the model using the client, send requests, handle rate limits (2 RPM / 50 per day free tier), and save results. Mentions prerequisites like an API key from Google AI Studio.\n* **Transcript:** So this notebook will help you to walk through that. So what are we going to do is to basically read the CSV file and use the, uh, genai.client to interact with the Gemini 2.5 Pro model. And we're going to send the request using this function, the models.generate_contents with some retry logic and print the... Okay. So you have to have the Gemini API set up from the Google AI Studio. Yeah, you can create an API key.\n\n**Cut Segment 7**\n* **Timestamp:** [13:44 - 15:18]\n* **Summary:** Explains the \"Configuration & Client Initialization\" and \"Handling API Requests with Retries\" sections of the Colab code. Shows setting the `MODEL_NAME` to `gemini-2.5-pro-exp-03-25`, initializing the `genai.Client`, and setting up rate limit constants (2 RPM, resulting in a 31-second delay). Shows the `send_request_with_retry` function using `client.models.generate_content`.\n* **Transcript:** Okay, so this is the place we set up the client with... Okay, so this is the place we name the model and we set up the Gen AI client. And with some rate limit settings for now. And... So then I have, you know, request functions being written here to make sure that it won't cause any, um, rate limits issues. And this part is the real part for sending the request with the contents which will be the prompt in a list with the model name. And yes.\n\n**Cut Segment 8**\n* **Timestamp:** [15:32 - 17:13]\n* **Summary:** Explains the \"Preparing the Data and Constructing the Prompt\" section of the Colab code. Shows the function to read the CSV data and the `rewrite_prompt` function, highlighting the specific prompt instructions given to the Gemini model to rewrite the original prompt while keeping all information and ensuring only the rewritten text is output.\n* **Transcript:** So next part, we have the CSV file function, um, rewriting the prompt function. So this part is our, um, you know, prompt instructions for Gemini to, um, you know, execute the tasks for us. So like in our case, like we, we want Gemini to rewrite the prompt. And but we're going to, like, we're going to tell the Gemini that rewrite the following prompt while keeping all the original information except the same. And we insert the prompt from the Pandas data frame. And we also have some guidelines here. So we keep all the information and the meaning the same thing. Do not add or remove any details. Do not change anything. Since like that. And we must tell that provide only the rewritten prompt so that it won't, you know, response additional information. So eventually we want to add this rewritten prompt into our data frame. Okay. Next, we have to define our main function and execute all the codes.\n\n**Cut Segment 9**\n* **Timestamp:** [18:00 - 19:52]\n* **Summary:** Demonstrates running the main execution cell in Colab. Shows the script starting, loading the 83 rows from `data.csv`, and beginning the prompt rewriting process. It displays Row 1's original prompt, the attempt to send the request to `gemini-2.5-pro-exp-03-25`, the successful response (rewritten prompt), and the enforced 31-second wait before processing the next row, confirming the rate limiting works as intended. Repeats for Row 2.\n* **Transcript:** Alright. Make sure that you have the CSV file uploaded to the notebook, the Colab. And finally, we run the prompt rewriting process. And we read the file and do that. So here you can see from the results... and we load 83 rows from the data CSV file. And it starts processing that. And we can check that the original prompt... So we can, it's really nice to have the, you know, process being printed out. So we have, we can check that the model send the request to the Gemini model 2.5 Pro. And then we got received the prompt which is the rewritten, um, version of the original prompt. So, you know, it's waiting properly like the waiting for 31 seconds before the next request. And then it goes to the next one. And has original prompt and the received the rewritten prompt, assume the role... which is pretty cool.\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title 6. Display Results\n",
        "if response:\n",
        "    display(Markdown(response.text))\n",
        "else:\n",
        "    print(\"No analysis results generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3gylU0B50Zr",
        "outputId": "17cd6f60-b407-47ce-a3f4-876bdd44810c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted file: files/99zfw6z8tkhr\n",
            "Deleted file: files/1j6vm4i8dgpn\n"
          ]
        }
      ],
      "source": [
        "# @title 7. Clean Up (Optional)\n",
        "def delete_file(client, file_obj):\n",
        "    if file_obj:\n",
        "        try:\n",
        "            client.files.delete(name=file_obj.name)\n",
        "            print(f\"Deleted file: {file_obj.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting file {file_obj.name}: {e}\")\n",
        "\n",
        "# Clean up both files\n",
        "delete_file(client, video_file)\n",
        "delete_file(client, ref_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0964497",
   "metadata": {},
   "source": [
    "### Notebook Walkthrough & References\n",
    "Prefer watching the flow? Here is the video companion for this notebook: [YouTube walkthrough](https://www.youtube.com/watch?v=w3aTT4kW318).\n",
    "\n",
    "Helpful references while you run the demo:\n",
    "- [LangGraph Agentic RAG guide](https://docs.langchain.com/oss/python/langgraph/agentic-rag)\n",
    "- [Ollama granite4 model card](https://ollama.com/library/granite4)\n",
    "- [EmbeddingGemma 300M on Hugging Face](https://huggingface.co/google/embeddinggemma-300m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773123d1",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "Install LangGraph, LangChain, and helper packages used throughout this demo.\n",
    "\n",
    "Need to finish setting up `uv` or Ollama first? I demo both in this video: [uv install @ 1:14](https://www.youtube.com/watch?v=LXSfjOCYD40&t=74s&pp=0gcJCdcCDuyUWbzu) and [Ollama install @ 1:50](https://www.youtube.com/watch?v=LXSfjOCYD40&t=110s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa42a411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: langgraph-rag-demo\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m141 packages\u001b[0m \u001b[2min 712ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)                                                 \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m     0 B/392.52 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 16.00 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 32.00 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 48.00 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 61.72 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 77.72 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 93.72 KiB/392.52 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 109.72 KiB/392.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 125.72 KiB/392.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 141.72 KiB/392.52 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 141.72 KiB/392.52 KiB       \u001b[1A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 141.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 141.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 157.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 173.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 189.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 30.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 189.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2mjiter               \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.92 KiB/311.14 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/113)------------\u001b[0m\u001b[0m 189.72 KiB/392.52 KiB       \u001b[2A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m113 packages\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m                                                \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/113] \u001b[2mInstalling wheels...                               \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to clone files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, reflinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m113 packages\u001b[0m \u001b[2min 5.85s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbcrypt\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbs4\u001b[0m\u001b[2m==0.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbuild\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mchromadb\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdurationpy\u001b[0m\u001b[2m==0.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==25.9.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.72.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.76.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkubernetes\u001b[0m\u001b[2m==33.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-chroma\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-huggingface\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-ollama\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.42\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mload-dotenv\u001b[0m\u001b[2m==0.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmmh3\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moauthlib\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.59b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moverrides\u001b[0m\u001b[2m==7.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==5.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypika\u001b[0m\u001b[2m==0.48.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyproject-hooks\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-oauthlib\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -U langgraph \"langchain[openai]\" langchain-community langchain-text-splitters bs4 langchain_huggingface load_dotenv langchain-chroma sentence_transformers langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687345fe",
   "metadata": {},
   "source": [
    "### Load Source Articles\n",
    "Fetch Lilian Weng blog posts with WebBaseLoader so we have raw content to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb850463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/Documents/langgraph-rag-agent/langgraph-rag-demo/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982556b",
   "metadata": {},
   "source": [
    "### Preview Raw Content\n",
    "Glance at the opening portion of the first document to confirm the loader worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da18d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet’s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to ac\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43debf",
   "metadata": {},
   "source": [
    "### Chunk Documents\n",
    "Split the articles into overlapping chunks that are easier to embed and retrieve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c95187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d742e",
   "metadata": {},
   "source": [
    "### Inspect First Chunk\n",
    "Print one chunk to understand the snippets the retriever will see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89cd6e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Hacking in Reinforcement Learning | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n"
     ]
    }
   ],
   "source": [
    "print(doc_splits[0].page_content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7d110",
   "metadata": {},
   "source": [
    "### Configure Embeddings\n",
    "Log into Hugging Face and initialize the EmbeddingGemma model used for vectorization.\n",
    "\n",
    "Replace `login(token='YOUR_HUGGINGFACE_API_TOKEN')` with your own token (loading it from an env var is safest) so the embedding model can authenticate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='YOUR_HUGGINGFACE_API_TOKEN')\n",
    "\n",
    "EMBEDDING_MODEL = 'google/embeddinggemma-300m'  # Google's new EmbeddingGemma model\n",
    "EMBEDDING_DIMS = 256  # Truncated from 768 for 3x faster processing\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    encode_kwargs={\"truncate_dim\": EMBEDDING_DIMS}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b66cb",
   "metadata": {},
   "source": [
    "### Build Vector Store\n",
    "Store the chunked documents in an in-memory vector index and expose a retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d184706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55840c3",
   "metadata": {},
   "source": [
    "### Create Retriever Tool\n",
    "Wrap the retriever as a LangChain tool so the agent can call it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f760c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc778dd",
   "metadata": {},
   "source": [
    "### Run Sample Retrieval\n",
    "Issue a reward-hacking query through the tool to fetch supporting passages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc444a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "res = retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234f561",
   "metadata": {},
   "source": [
    "### Display Retrieved Chunks\n",
    "Pretty-print the retrieval result to inspect what was returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d98ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Detecting Reward Hacking#\\n'\n",
      " '\\n'\n",
      " '(Note: Some work defines reward tampering as a distinct category of '\n",
      " 'misalignment behavior from reward hacking. But I consider reward hacking as '\n",
      " 'a broader concept here.)\\n'\n",
      " 'At a high level, reward hacking can be categorized into two types: '\n",
      " 'environment or goal misspecification, and reward tampering.\\n'\n",
      " '\\n'\n",
      " 'Why does Reward Hacking Exist?#\\n'\n",
      " '\\n'\n",
      " 'In-Context Reward Hacking#')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43471578",
   "metadata": {},
   "source": [
    "### Configure Response Model\n",
    "Set up a ChatOllama model plus message/state types that power the agent.\n",
    "\n",
    "Reminder: make sure Ollama is installed, the service is running locally, and the `granite4:350m` model (or whichever you pick) is already pulled—see the video segment above if you need a refresher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7eb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "response_model = ChatOllama(\n",
    "    model=\"granite4:350m\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbcdb8",
   "metadata": {},
   "source": [
    "### Define Generate-or-Respond Node\n",
    "Create the node that either answers directly or requests retrieval based on the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f092961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])  \n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f3005",
   "metadata": {},
   "source": [
    "### Test Direct Response Path\n",
    "Probe the node with a simple greeting to ensure it can reply without retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48011f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm a helpful assistant with access to certain tools. Could you please provide more details about what you need assistance with? I can look up blog posts or perform other tasks using the functions available to me.\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e2f9d",
   "metadata": {},
   "source": [
    "### Test Retrieval-triggering Question\n",
    "Send a domain question to confirm the model issues a retriever tool call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "254b6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_blog_posts (738041e7-d93d-411a-a856-db91c35721a3)\n",
      " Call ID: 738041e7-d93d-411a-a856-db91c35721a3\n",
      "  Args:\n",
      "    query: types of reward hacking\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fedbb",
   "metadata": {},
   "source": [
    "### Set Up Document Grader\n",
    "Define the relevance-grading prompt, schema, and logic that decide if context is good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850664fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):  \n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = ChatOllama(\n",
    "    model=\"granite4:350m\",\n",
    "    # model=\"llama3.2:1b\",\n",
    "    # model=\"qwen3:0.6b\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0,)\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(  \n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13da45f",
   "metadata": {},
   "source": [
    "### Grade Irrelevant Context\n",
    "Run the grader on nonsense tool output to watch it route toward question rewriting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7c9085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b145f3",
   "metadata": {},
   "source": [
    "### Grade Relevant Context\n",
    "Run the grader on a good snippet to verify it green-lights answer generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6f462e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate_answer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1dbc9",
   "metadata": {},
   "source": [
    "### Define Question Rewriter\n",
    "Add a prompt/function that rewrites the user query before rerunning retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a596b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1563e",
   "metadata": {},
   "source": [
    "### Test Question Rewriter\n",
    "Pass in a mocked trace to see how the question is rephrased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68b38f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we better understand Lilian Weng's perspective on the various types of reward hacking?\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fd4cf",
   "metadata": {},
   "source": [
    "### Define Answer Generator\n",
    "Create the concise answer prompt/function that consumes question plus retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "319e3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Retrieved Context: \\n'''{context}\\n'''\\n\"\n",
    "    \"Question: \\n{question} \\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f861f",
   "metadata": {},
   "source": [
    "### Test Answer Generator\n",
    "Feed in a mocked retrieval response to ensure the agent can craft a final reply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dca0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lilian Weng categorizes reward hacking into two types: environment or goal mis-specification and reward tampering.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Lilian Weng says reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3b2cc",
   "metadata": {},
   "source": [
    "### Assemble LangGraph Workflow\n",
    "Wire all nodes, tools, and routing logic into a LangGraph state machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981b687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4a4c9",
   "metadata": {},
   "source": [
    "### Stream End-to-end Run\n",
    "Execute the compiled graph on a sample question and view each node's updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8cfaee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node generate_query_or_respond\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_blog_posts (d66958a1-ee7f-4830-a0c6-7704cf750506)\n",
      " Call ID: d66958a1-ee7f-4830-a0c6-7704cf750506\n",
      "  Args:\n",
      "    query: types of reward hacking\n",
      "\n",
      "\n",
      "\n",
      "Update from node retrieve\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_blog_posts\n",
      "\n",
      "Detecting Reward Hacking#\n",
      "\n",
      "(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\n",
      "At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n",
      "\n",
      "Why does Reward Hacking Exist?#\n",
      "\n",
      "In-Context Reward Hacking#\n",
      "\n",
      "\n",
      "\n",
      "Update from node generate_answer\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lilian Weng states that reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langgraph-rag-demo)",
   "language": "python",
   "name": "langgraph-rag-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

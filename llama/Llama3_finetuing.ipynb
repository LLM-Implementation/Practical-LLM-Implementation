{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJKdWbLYtNdw"
      },
      "source": [
        "# Fine-tuning Llama 3 with Custom CSV Data\n",
        "\n",
        "This notebook demonstrates how to fine-tune Llama 3 models using custom CSV data. We'll cover:\n",
        "1. Setting up the environment\n",
        "2. Loading and formatting our CSV data\n",
        "3. Creating a properly formatted dataset for Llama 3\n",
        "4. Fine-tuning with LoRA (Low-Rank Adaptation)\n",
        "5. Testing the fine-tuned model\n",
        "\n",
        "This approach is simpler than the standard Unsloth tutorial, focusing specifically on custom CSV data processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UqPP07BtNdx"
      },
      "source": [
        "## 1. Setting Up the Environment\n",
        "\n",
        "First, let's install the necessary libraries. We're using Unsloth which provides optimized training for Llama models."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdOalm8jwxGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INqRc14HtNdx",
        "outputId": "0b45ff09-e247-4c5d-952f-82badc7295f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 trl-0.16.1 unsloth_zoo-2025.3.17 xformers-0.0.29.post3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, hf_transfer, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.3.17 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth-zoo 2025.3.17 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.16.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf_transfer-0.1.9 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.3.19-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.3.19-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.3.19\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0gBKWeYtNdy"
      },
      "source": [
        "## 2. Load and Examine the CSV Data\n",
        "\n",
        "Now let's load our CSV file which contains prompts and their rewritten versions. We'll examine the structure to understand how to format it for Llama 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "AbOqqBcPtNdy",
        "outputId": "c6827377-e2f2-4dfd-c361-ee177b1193c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 83 rows\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Using WebPilot, create an outline for an artic...   \n",
              "1  I want you to act as an English translator, sp...   \n",
              "2  I want you to act as an interviewer. I will be...   \n",
              "3  I want you to act as a javascript console. I w...   \n",
              "4  I want you to act as a text based excel. you'l...   \n",
              "\n",
              "                                      gemini_rewrite  \n",
              "0  Craft a detailed article outline (for a 2,000-...  \n",
              "1  I want you to act as an English translator, sp...  \n",
              "2  Assume the role of an interviewer. I will be t...  \n",
              "3  Simulate a JavaScript console. Respond to my c...  \n",
              "4  I want you to act as a text-based Excel. You'l...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d180d47-f76c-44e9-83c5-5177a68c3df4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>gemini_rewrite</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Using WebPilot, create an outline for an artic...</td>\n",
              "      <td>Craft a detailed article outline (for a 2,000-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I want you to act as an English translator, sp...</td>\n",
              "      <td>I want you to act as an English translator, sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I want you to act as an interviewer. I will be...</td>\n",
              "      <td>Assume the role of an interviewer. I will be t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I want you to act as a javascript console. I w...</td>\n",
              "      <td>Simulate a JavaScript console. Respond to my c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I want you to act as a text based excel. you'l...</td>\n",
              "      <td>I want you to act as a text-based Excel. You'l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d180d47-f76c-44e9-83c5-5177a68c3df4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d180d47-f76c-44e9-83c5-5177a68c3df4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d180d47-f76c-44e9-83c5-5177a68c3df4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-185b5714-deb4-41aa-bed1-0fd93787308c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-185b5714-deb4-41aa-bed1-0fd93787308c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-185b5714-deb4-41aa-bed1-0fd93787308c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \\\"istanbulu cok seviyom burada olmak cok guzel\\\"\",\n          \"I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. First, reply me the empty sheet.\",\n          \"I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conversation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \\\"Hi\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gemini_rewrite\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \\\"istanbulu cok seviyom burada olmak cok guzel\\\"\\n\",\n          \"I want you to act as a text-based Excel. You'll only reply with a text-based, 10-row Excel sheet, with row numbers and cell letters as columns (A to L). The first column header should be empty, to serve as a reference for the row number. I will tell you what to write into the cells, and you will only reply with the result of the Excel table as text, nothing else. Do not include any explanations. I will write formulas, you will execute them, and you will only reply with the result of the Excel table as text. First, reply with the empty sheet.\\n\",\n          \"Assume the role of an interviewer. I will be the candidate, and you will interview me for the `position` position. Your responses should be solely those of an interviewer. Conduct the interview one question at a time; do not provide the entire conversation at once. Only interact with me as the interviewer. Pose the questions, await my responses, and refrain from providing explanations. Proceed with each question individually, just as an interviewer would, and wait for my replies. My initial statement is \\\"Hi\\\".\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Dataset has {len(df)} rows\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trTaipb6tNdy"
      },
      "source": [
        "## 3. Load the Llama 3 Model\n",
        "\n",
        "We'll load the base Llama 3 model. For this tutorial, we're using the smaller 1B parameter model which is faster to fine-tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "d3cd2a80c87d470386cf436d6a47f6ea",
            "2662f00289c94569a379c00784c3055a",
            "feb1158ea2f84b4c81333965c1f160ec",
            "8cd00cdea99c4efc9c17591912b85a23",
            "2d5b776845074a61a76ab5a0008f90ec",
            "ea4e1b06d7e64f38866fc9c8245819f5",
            "763d2fa663a44fe5831ebee4c507ef16",
            "3390da5d3482407896248b423a5c70b7",
            "12fab82e403340fa8cf134f4324e3bf1",
            "8826123b892a477b9a7c13feb32aa06f",
            "33eed1a26c244c63952353c3f08fce13",
            "f6e962ad1780462bbc481f9ba4af7983",
            "9ed15e26d47242ea844aab6f7dc5990e",
            "2a28bfac8e7441f2816f21a043d424d6",
            "208e6cc5def5450c9d431be1f39ccac9",
            "73fa50ed8b68401bb5b592e7e4383b6f",
            "f09b0955f88d45eda1268c9349b1f6c6",
            "44c8ae4bdfc14097b9356f9112739a02",
            "814ce3b5227448958d468eee73ac19b8",
            "02347e940c2344fe8df84f441c9fcd8d",
            "8a44062c58f84c42a59ec8a1995011f3",
            "1daffc093baa4ed1aee44d59ef464fdf",
            "e0d79fcae38c43c3ae2c74aaad4d4db7",
            "8a9cb35f95f04844bf5da6f775cdcf6b",
            "b55f15985e77410383278c780a3f4538",
            "448b5f2ba1834a869eac127a10f7069e",
            "c0134c442fa747ed9e6f4062f882ffaf",
            "478e8ef5a1dd407f91d43837e321bd0e",
            "af2c035080314603bd369c04039a4380",
            "b8ed67d1c6b443c08ab7aac0ea028c7b",
            "fb98c087ca2847e08c2ea3eb767b76e7",
            "1848351e97104a26b55e26ae740bb437",
            "19f9536254a64195848343aace9297d5",
            "482a782254ab4954a5bb1ffe28a64314",
            "42bb13755dda4ff49d20cdcb3402c1ec",
            "8d8ff9f507314547824cbdf85cb7edf7",
            "d3924b7be763486fa3b8f925dabfe425",
            "f4037d15bdc74fb5ae743578264665b3",
            "72a9030daa4c4bf096c4a786cf005f84",
            "fa9cf184572745e1b2e849b8ffff00fa",
            "ae5f3305086d405e86c3dea6b68eb054",
            "96dd62202df24a51990346ccf6bd4a1c",
            "44d1e13682d644b8afe742de51ee7501",
            "b624d4805083406f9719d57786ec6013",
            "258838a5166544efa3b806683e4c7d04",
            "5787d23c4990498c8a59131144493b3e",
            "ef189251564d4d07a23efb5808535e39",
            "bcd3bc16534b490992a028cfaa9ac96a",
            "0d4901d0f840458e9d8bbabd4cf5b543",
            "72cef29db6124c8c9237a03bf2bf878c",
            "b8236a5efdbb46b1936876838bd0eb17",
            "7574c585403e492caafe144af9db5243",
            "eb469e967f86444b920e91e64e31bb29",
            "555127494708402aad0300416d3d5639",
            "b36cc08a686f4d5c879e0720b3b3f9b1"
          ]
        },
        "id": "UvQuMG1qtNdy",
        "outputId": "e60492f7-6e8f-44d9-8c28-6352c1094cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3cd2a80c87d470386cf436d6a47f6ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6e962ad1780462bbc481f9ba4af7983"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0d79fcae38c43c3ae2c74aaad4d4db7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "482a782254ab4954a5bb1ffe28a64314"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "258838a5166544efa3b806683e4c7d04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 8192\n",
        "load_in_4bit = True  # Use 4-bit quantization to reduce memory usage\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",  # Using the smaller model for speed\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # Uncomment if using gated models\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4u2CM0htNdy"
      },
      "source": [
        "## 4. Add LoRA Adapters\n",
        "\n",
        "We'll use LoRA (Low-Rank Adaptation) to efficiently fine-tune the model. This allows us to only update a small percentage of parameters, making fine-tuning faster and more memory-efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmeR5uxgtNd0",
        "outputId": "94cc7a2f-85b9-468d-c73d-0b68bf5cd28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA adapters added to the model.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Rank of the adaptation matrices\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Uses less VRAM\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added to the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHOD7TLWtNd1"
      },
      "source": [
        "## 5. Format Data for Llama 3\n",
        "\n",
        "Now we'll format our CSV data to match the Llama 3 chat template. Instead of using the built-in template functions, we'll create our own formatter that directly applies the Llama 3 formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBOLlEZ3tNd1",
        "outputId": "6ad96a74-e2b2-44cf-a5c9-c9d291ce4a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 83 examples\n",
            "\n",
            "Example of formatted text:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Your task is to rewrite AI-generated prompts to make them more human-like.\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Craft a detailed article outline (for a 2,000-word article) on the keyword 'Best SEO prompts', leveraging the top 10 Google results via WebPilot. Ensure the outline is comprehensive enough to facilitate the creation of the full 2,000-word article.\n",
            "\n",
            "**Part 1 & Part 2 Outline Structure:**\n",
            "\n",
            "*   Include every relevant heading, maintaining a high keyword density within the headings.\n",
            "*   Specify the word count for each section of the outline.\n",
            "*   Incorporate a FAQs section, drawing from Google's \"People also ask\" section for the keyword 'Best SEO prompts'.\n",
            "\n",
            "**Keyword & Related Term Generation:**\n",
            "\n",
            "*   Compile an extensive list of LSI and NLP keywords associated with 'Best SEO prompts'.\n",
            "*   List any other words related to the keyword.\n",
            "\n",
            "**External Linking Recommendations:**\n",
            "\n",
            "*   Suggest 3 relevant external links (non-competing articles).\n",
            "*    Provide the recommended anchor text for each link.\n",
            "\n",
            "Split the outline into part 1 and part 2.\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they‚Äôre not competing articles. Split the outline into part 1 and part 2.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def format_as_llama3(row):\n",
        "    \"\"\"\n",
        "    Format a row from our CSV into the Llama 3 chat format:\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "    System instruction\n",
        "\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    User prompt\n",
        "\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "    Assistant response<|eot_id|>\n",
        "    \"\"\"\n",
        "    # System message explaining the task\n",
        "    system_msg = \"Your task is to rewrite AI-generated prompts to make them more human-like.\"\n",
        "\n",
        "    # User message is the AI-generated text\n",
        "    user_msg = row['gemini_rewrite']\n",
        "\n",
        "    # Assistant message is the human-written text\n",
        "    assistant_msg = row['prompt']\n",
        "\n",
        "    # Format in Llama 3 template\n",
        "    formatted_text = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{assistant_msg}<|eot_id|>\"\n",
        "\n",
        "    return formatted_text\n",
        "\n",
        "# Apply our formatter to each row in the dataframe\n",
        "formatted_texts = df.apply(format_as_llama3, axis=1).tolist()\n",
        "\n",
        "# Create a HuggingFace dataset\n",
        "dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
        "\n",
        "print(f\"Dataset created with {len(dataset)} examples\")\n",
        "print(\"\\nExample of formatted text:\")\n",
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csJyBWiLtNd1"
      },
      "source": [
        "## 6. Configure Training\n",
        "\n",
        "Now we'll set up the training configuration using TRL's SFTTrainer, which makes it easy to fine-tune language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b5065606cce9463e89bd5dd6023212aa",
            "f40ca7a899dc41c293609d160ce5f9c9",
            "ecca36135b5a4876952517d5bd892600",
            "ce78b4bcb5a5419bb5a8366e76fb6a91",
            "f738af31cbf94f64af542b64084f77d9",
            "6fb5f9461e304e3aac72fb6fdd037208",
            "cfcdc6d42e604d8d96c870ecc1c5f298",
            "671d10e8cc634926a260956e9b8ac44d",
            "c4c4fef6b91a4bdd9d4989fdb4defc7e",
            "fe02d178b77542b7a4e43b626794f300",
            "1a1934d9f76d45cda61cf96a7d9db99c"
          ]
        },
        "id": "_5-PHSoDtNd1",
        "outputId": "8e069494-7e24-43ef-dc7c-b3a570c8e414"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/83 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5065606cce9463e89bd5dd6023212aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration complete!\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama3_finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs = 2, # Set this for 1 full training run.\n",
        "    # max_steps=60,\n",
        "    learning_rate= 2e-4,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    report_to=\"none\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    packing=False,  # Set to True for shorter sequences to speed up training\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Training configuration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhUEkSZtNd1"
      },
      "source": [
        "## 7. Train the Model\n",
        "\n",
        "Now let's train our model! This will fine-tune the Llama 3 model on our custom data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "id": "hxkEtKybtNd1",
        "outputId": "cb9720ae-24e2-4f2d-bf95-78fc4147d089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "1.088 GB of memory reserved.\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 83 | Num Epochs = 2 | Total steps = 20\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:32, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.299100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.841100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.054400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.606400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.049100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.558000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.447600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.993900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.196000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.901600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.864500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.094400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.153600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.998800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.824400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.164400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.992700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.660100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete in 41.2026 seconds\n",
            "(0.69 minutes)\n"
          ]
        }
      ],
      "source": [
        "# Optional: setup to monitor GPU usage\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Print training stats\n",
        "print(f\"\\nTraining complete in {trainer_stats.metrics['train_runtime']} seconds\")\n",
        "print(f\"({round(trainer_stats.metrics['train_runtime']/60, 2)} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CirabWDBtNd1"
      },
      "source": [
        "## 8. Save the Model\n",
        "\n",
        "Now we'll save our fine-tuned model for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beMgJMFhtNd1",
        "outputId": "68d3d940-d22f-4d1a-a7b5-ceaabbd4f5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to llama3_humanizing_finetuned\n"
          ]
        }
      ],
      "source": [
        "# Save the model locally\n",
        "output_dir = \"llama3_humanizing_finetuned\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "# Uncomment to save to Hugging Face Hub\n",
        "# model.push_to_hub(\"your-username/llama3-humanizing\", token=\"your_hf_token\")\n",
        "# tokenizer.push_to_hub(\"your-username/llama3-humanizing\", token=\"your_hf_token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKKUuu7utNd1"
      },
      "source": [
        "## 9. Test the Fine-tuned Model\n",
        "\n",
        "Finally, let's test our model to see how well it performs!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"llama3_humanizing_finetuned\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        load_in_4bit = load_in_4bit\n",
        "    )\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "llama3_chatformat = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{}\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{}<|eot_id|>\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    llama3_chatformat.format(\n",
        "        \"Your task is to rewrite AI-generated prompts to make them more human-like.\", # instruction\n",
        "        \"Assume you are a helpful assistant explaining a process. I am learning. Explain the basic steps for fine-tuning a language model like Llama 3, one step at a time. Wait for me to say 'Okay' before you tell me the next step. Start with the very first step.\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5MqWcXicnMl",
        "outputId": "093ea8ef-395d-40cf-aa15-947a5acb8113"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Your task is to rewrite AI-generated prompts to make them more human-like.\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Assume you are a helpful assistant explaining a process. I am learning. Explain the basic steps for fine-tuning a language model like Llama 3, one step at a time. Wait for me to say 'Okay' before you tell me the next step. Start with the very first step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I want you to act as a helpful assistant. I am learning. Explain the basic steps for fine-tuning a language model like Llama 3, one step at a time. Wait for me to say 'Okay' before you tell me the next step. Start with the very first step.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4K5SkCoft7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWX78q51tNd2"
      },
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "Congratulations! You've successfully:\n",
        "1. Loaded and formatted custom CSV data for Llama 3 fine-tuning\n",
        "2. Applied the correct Llama 3 chat template format\n",
        "3. Fine-tuned a Llama 3 model with LoRA\n",
        "4. Saved and tested your fine-tuned model\n",
        "\n",
        "This notebook provides a simplified approach to fine-tuning Llama 3 models with custom data, focusing specifically on the humanizing prompts task."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
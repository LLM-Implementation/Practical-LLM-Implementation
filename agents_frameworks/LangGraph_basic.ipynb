{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview_section"
      },
      "source": [
        "# Overview\n",
        "\n",
        "LangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\n",
        "\n",
        "* **Reliability and controllability.** Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\n",
        "* **Low-level and extensible.** Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\n",
        "* **First-class streaming support.** With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learn_basics_section"
      },
      "source": [
        "# Learn LangGraph basics\n",
        "\n",
        "To get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\n",
        "\n",
        "* Build a basic chatbot\n",
        "* Add tools\n",
        "* Add memory\n",
        "* Add human-in-the-loop controls\n",
        "* Customize state\n",
        "* Time travel\n",
        "\n",
        "In completing this series of tutorials, you will build a support chatbot in LangGraph that can:\n",
        "\n",
        "‚úÖ Answer common questions by searching the web\n",
        "‚úÖ Maintain conversation state across calls\n",
        "‚úÖ Route complex queries to a human for review\n",
        "‚úÖ Use custom state to control its behavior\n",
        "‚úÖ Rewind and explore alternative conversation paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basic_chatbot_intro"
      },
      "source": [
        "# Build a basic chatbot\n",
        "\n",
        "In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Let‚Äôs dive in! üåü\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as OpenAI, Anthropic, or Google Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_packages_markdown"
      },
      "source": [
        "## 1. Install packages\n",
        "\n",
        "Install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54c022b2-98c1-4328-ea41-0d49a7814e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.42)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.59)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.4)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Downloading langgraph-0.4.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, ormsgpack, langgraph-sdk, langgraph-checkpoint, google-ai-generativelanguage, langgraph-prebuilt, langchain_google_genai, langgraph\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.4 langgraph-0.4.7 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.1 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "837da53ce42b45a19a9cb6881369f0dc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U langgraph langsmith langchain langchain_google_genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "langsmith_tip"
      },
      "source": [
        "**Tip**: Sign up for [LangSmith](https://smith.langchain.com/) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see [LangSmith docs](https://docs.smith.langchain.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_stategraph_markdown"
      },
      "source": [
        "## 2. Create a StateGraph\n",
        "\n",
        "Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages.\n",
        "\n",
        "Start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a \"state machine\". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions.\n",
        "\n",
        "**API Reference:** [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph) | [START](https://langchain-ai.github.io/langgraph/reference/graphs/#start) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#add_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_stategraph_code"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stategraph_explanation"
      },
      "source": [
        "Our graph can now handle two key tasks:\n",
        "\n",
        "* Each `node` can receive the current `State` as input and output an update to the state.\n",
        "* Updates to `messages` will be appended to the existing list rather than overwriting it, thanks to the prebuilt `add_messages` function used with the `Annotated` syntax.\n",
        "\n",
        "**Concept**: When defining a graph, the first step is to define its `State`. The `State` includes the graph's schema and reducer functions that handle state updates. In our example, `State` is a `TypedDict` with one key: `messages`. The `add_messages` reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values. To learn more about state, reducers, and related concepts, see [LangGraph reference docs](https://langchain-ai.github.io/langgraph/concepts/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_node_markdown"
      },
      "source": [
        "## 3. Add a node\n",
        "\n",
        "Next, add a \"chatbot\" node. `Nodes` represent units of work and are typically regular Python functions.\n",
        "\n",
        "Let's first select and set up a chat model (we'll use Google Gemini here). You will need to get a [Google API Key](https://ai.google.dev/pricing) and set it as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llm_setup_code"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Get your API key (replace with your actual key or use a secure method)\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n",
        "\n",
        "# Or use getpass for a more secure input method in Colab\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")\n",
        "\n",
        "# Initialize the Chat Model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chatbot_node_markdown"
      },
      "source": [
        "We can now incorporate the chat model into a simple node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chatbot_node_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78fa010-7ead-4f3f-d040-66ce631f5c58",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b7dc19e0390>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chatbot_node_explanation"
      },
      "source": [
        "Notice how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key \"messages\". This is the basic pattern for all LangGraph node functions.\n",
        "\n",
        "The `add_messages` function in our `State` will append the LLM's response messages to whatever messages are already in the state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_entry_point_markdown"
      },
      "source": [
        "## 4. Add an entry point\n",
        "\n",
        "Add an `entry` point to tell the graph where to start its work each time it is run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_entry_point_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbee9f6-6c43-4093-fca1-588a918dfb84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b7dc19e0390>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "graph_builder.add_edge(START, \"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compile_graph_markdown"
      },
      "source": [
        "## 5. Compile the graph\n",
        "\n",
        "Before running the graph, we'll need to compile it. We can do so by calling `compile()` on the graph builder. This creates a `CompiledGraph` we can invoke on our state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compile_graph_code"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_graph_markdown"
      },
      "source": [
        "## 6. Visualize the graph (optional)\n",
        "\n",
        "You can visualize the graph using the get_graph method and one of the \"draw\" methods, like draw_ascii or draw_png. The draw methods each require additional dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_graph_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "aa6520bf-b962-4b7d-89d8-432a1544bb72"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_chatbot_markdown"
      },
      "source": [
        "## 7. Run the chatbot\n",
        "\n",
        "Now run the chatbot! We'll use the `stream` method to see the updates as they happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run_chatbot_code",
        "outputId": "fd3c3b92-91f2-47d7-ef0b-8d6e13a6d39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:1501: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What do you know about LangGraph?\n",
            "Assistant: LangGraph is a library that extends the capabilities of LangChain by providing a way to build **robust and stateful multi-actor applications** using a **graph-based approach**.\n",
            "\n",
            "Essentially, while LangChain provides the building blocks (LLMs, tools, chains, agents), LangGraph gives you a powerful framework to **orchestrate complex sequences of operations, manage state, and handle intricate control flow**, especially for applications that require multiple steps, conditional logic, and iterative processes.\n",
            "\n",
            "Here's a breakdown of what LangGraph is and what it offers:\n",
            "\n",
            "### Core Concepts\n",
            "\n",
            "1.  **State:** At the heart of LangGraph is the concept of `State`. This is a shared object that gets passed between different nodes in your graph. Any node can read from or write to this state, allowing information to persist and evolve across multiple steps of your application. This is crucial for multi-turn conversations, agents that need to remember previous actions, or complex pipelines.\n",
            "\n",
            "2.  **Nodes:** These are the individual units of work in your graph. A node can be:\n",
            "    *   An LLM call (e.g., to generate a response).\n",
            "    *   A tool invocation (e.g., searching the web, calling an API).\n",
            "    *   A custom Python function (e.g., processing data, making a decision).\n",
            "    *   Even another LangChain chain or agent.\n",
            "\n",
            "3.  **Edges:** Edges define the transitions between nodes. They dictate how the execution flow moves from one node to the next.\n",
            "    *   **Direct Edges:** Unconditionally move from one node to another.\n",
            "    *   **Conditional Edges:** The most powerful type. They allow you to define transitions based on the *output* of a node. For example, if a node's output indicates a tool is needed, you can transition to a \"tool_executor\" node; otherwise, you might transition to a \"final_answer\" node.\n",
            "\n",
            "4.  **Graphs and Cycles:** LangGraph allows you to define a directed graph, complete with entry and exit points. Crucially, it supports **cycles (loops)**. This is a significant differentiator from simple sequential chains, enabling:\n",
            "    *   **Self-correction:** An agent can try an action, observe the result, and if it's not satisfactory, loop back to a previous step to try again or refine its approach.\n",
            "    *   **Iterative refinement:** Repeating a process until a condition is met (e.g., generating code, testing it, and fixing errors until it works).\n",
            "    *   **Human-in-the-loop:** Pausing execution for human input or approval before proceeding.\n",
            "\n",
            "### Why Use LangGraph? (Key Advantages)\n",
            "\n",
            "*   **Robust Orchestration:** Build highly resilient applications that can handle complex logic, errors, and conditional flows gracefully.\n",
            "*   **State Management:** Explicitly manage and pass state between steps, essential for multi-turn interactions and long-running processes.\n",
            "*   **Complex Agentic Behavior:** Ideal for building sophisticated LLM agents that can:\n",
            "    *   Reason and plan.\n",
            "    *   Use multiple tools.\n",
            "    *   Self-correct and iterate.\n",
            "    *   Engage in multi-step problem-solving.\n",
            "*   **Human-in-the-Loop:** Easily integrate points where human intervention or approval is required.\n",
            "*   **Debugging and Visibility:** The graph structure makes it easier to visualize the flow, trace execution paths, and debug complex interactions.\n",
            "*   **Flexibility:** Nodes can be anything from simple functions to complex LangChain components, offering immense flexibility.\n",
            "\n",
            "### Relationship to LangChain\n",
            "\n",
            "LangGraph is built *on top of* LangChain and is considered an integral part of the LangChain ecosystem.\n",
            "*   **LangChain provides the primitives:** LLMs, Chains, Tools, Agents, Document Loaders, Retrievers, etc.\n",
            "*   **LangGraph provides the orchestration layer:** It takes these LangChain primitives and allows you to define how they interact in a structured, stateful, and often cyclical manner.\n",
            "\n",
            "You'll typically define your LLMs, tools, and potentially some simpler chains using LangChain, and then use LangGraph to wire them together into a sophisticated application flow.\n",
            "\n",
            "### Common Use Cases\n",
            "\n",
            "*   **Advanced LLM Agents:** Creating agents that can plan, execute tools, observe results, and self-correct (e.g., a \"research agent\" that iteratively queries, summarizes, and refines its search).\n",
            "*   **Multi-turn Chatbots:** Building conversational AI that remembers context, handles complex user intents requiring multiple steps, and can clarify information.\n",
            "*   **Human-in-the-Loop Workflows:** Applications where an LLM generates a draft, but a human needs to review or approve it before the next step.\n",
            "*   **Complex Data Processing Pipelines:** Automating workflows that involve conditional data transformation, validation, and iterative refinement.\n",
            "*   **Retrieval Augmented Generation (RAG) with Self-Correction:** A RAG system that, if an initial retrieval isn't sufficient, can re-query, re-rank, or refine its approach before generating a final answer.\n",
            "\n",
            "In summary, if you're building anything beyond a simple, linear LLM application and need to manage state, handle conditional logic, or enable iterative/self-correcting behavior, LangGraph is the tool in the LangChain ecosystem you'll want to leverage.\n"
          ]
        }
      ],
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "congrats_next_steps"
      },
      "source": [
        "## Congratulations! üéâ\n",
        "\n",
        "You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll add a web search tool to expand the bot's knowledge and make it more capable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_tools_intro"
      },
      "source": [
        "# Add tools üõ†Ô∏è\n",
        "\n",
        "To handle queries your chatbot can't answer \"from memory\", integrate a web search tool. The chatbot can use this tool to find relevant information and provide better responses.\n",
        "\n",
        "**Note**: This tutorial builds on *Build a basic chatbot*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_tools_prereqs"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before you start this tutorial, ensure you have the following:\n",
        "\n",
        "* An API key for the [Tavily Search Engine](https://tavily.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_tavily_markdown"
      },
      "source": [
        "## 1. Install the search engine\n",
        "\n",
        "Install the requirements to use the Tavily Search Engine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_tavily_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee25826-1b60-43bc-8954-0be021b071b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-tavily\n",
            "  Downloading langchain_tavily-0.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (3.11.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.25)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (0.3.59)\n",
            "Collecting mypy<2.0.0,>=1.15.0 (from langchain-tavily)\n",
            "  Downloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (4.13.2)\n",
            "Collecting mypy_extensions>=1.0.0 (from mypy<2.0.0,>=1.15.0->langchain-tavily)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2025.4.26)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.3.1)\n",
            "Downloading langchain_tavily-0.2.0-py3-none-any.whl (23 kB)\n",
            "Downloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy_extensions, mypy, langchain-tavily\n",
            "Successfully installed langchain-tavily-0.2.0 mypy-1.15.0 mypy_extensions-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configure_env_markdown"
      },
      "source": [
        "## 2. Configure your environment\n",
        "\n",
        "Configure your environment with your search engine API key. We use `getpass` here for secure input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configure_env_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec69152b-1dbc-41df-a5aa-c91859ef3d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TAVILY_API_KEY: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "define_tool_markdown"
      },
      "source": [
        "## 3. Define the tool\n",
        "\n",
        "Define the web search tool.\n",
        "\n",
        "**API Reference:** [TavilySearch](https://python.langchain.com/v0.1/docs/integrations/tools/tavily_search/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "define_tool_code",
        "outputId": "623a342d-6d8b-476b-82a7-9e1e7ad57b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's a 'node' in LangGraph?\",\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
              "   'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
              "   'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.',\n",
              "   'score': 0.7065353,\n",
              "   'raw_content': None},\n",
              "  {'title': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges',\n",
              "   'url': 'https://medium.com/@vivekvjnk/langgraph-basics-understanding-state-schema-nodes-and-edges-77f2fd17cae5',\n",
              "   'content': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges | by Story_Teller | Medium LangGraph Basics: Understanding State, Schema, Nodes, and Edges LangGraph Basics: Understanding State, Schema, Nodes, and Edges These predefined structures in the messaging app are synonymous with the schema of the state in LangGraph. Just as a messaging app ensures all interactions (messages) follow a consistent format, the schema in LangGraph ensures the state passed along edges is structured and interpretable. This static schema allows nodes to rely on a consistent state format, ensuring seamless communication along edges throughout the graph. In this article, we explored the foundational concepts of graph-based systems, drawing parallels to familiar messaging applications to illustrate how edges, nodes, and state transitions function seamlessly in dynamic workflows.',\n",
              "   'score': 0.6501347,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 2.11}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "define_graph_markdown"
      },
      "source": [
        "## 4. Define the graph\n",
        "\n",
        "For the `StateGraph` you created in the first tutorial, add `bind_tools` on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine.\n",
        "\n",
        "Let's first select our LLM (Google Gemini) and set the API key if you haven't already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_graph_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e252cad-4084-484b-8263-41b36b6f72a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b7db3ea3910>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Set up LLM API Key if needed\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
        "\n",
        "# Bind the tools to the LLM\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# Define the State\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Define the chatbot node (now with tool awareness)\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_tool_func_markdown"
      },
      "source": [
        "## 5. Create a function to run the tools\n",
        "\n",
        "Now, create a function to run the tools if they are called. We'll create a `BasicToolNode` class that checks the most recent message in the state and calls tools if the message contains `tool_calls`. It relies on the LLM's tool_calling support.\n",
        "\n",
        "**API Reference:** [ToolMessage](https://langchain-ai.github.io/langgraph/reference/messages/#toolmessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tool_func_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce2cff8-9370-47eb-bee9-d06cb901034b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToolNode added to the graph.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# class BasicToolNode:\n",
        "#     \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "#     def __init__(self, tools: list) -> None:\n",
        "#         self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "#     def __call__(self, inputs: dict):\n",
        "#         if messages := inputs.get(\"messages\", []):\n",
        "#             message = messages[-1]\n",
        "#         else:\n",
        "#             raise ValueError(\"No message found in input\")\n",
        "#         outputs = []\n",
        "#         for tool_call in message.tool_calls:\n",
        "#             tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "#                 tool_call[\"args\"]\n",
        "#             )\n",
        "#             outputs.append(\n",
        "#                 ToolMessage(\n",
        "#                     content=json.dumps(tool_result),\n",
        "#                     name=tool_call[\"name\"],\n",
        "#                     tool_call_id=tool_call[\"id\"],\n",
        "#                 )\n",
        "#             )\n",
        "#         return {\"messages\": outputs}\n",
        "\n",
        "# tool_node = BasicToolNode(tools=[tool])\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "print(\"ToolNode added to the graph.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ig6NoxMx55eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toolnode_note"
      },
      "source": [
        "**Note**: If you do not want to build this yourself in the future, you can use LangGraph's prebuilt `ToolNode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "define_edges_markdown"
      },
      "source": [
        "## 6. Define the conditional edges\n",
        "\n",
        "With the tool node added, now you can define the `conditional_edges`.\n",
        "\n",
        "*Edges* route the control flow from one node to the next. *Conditional edges* start from a single node and usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph `state` and return a string or list of strings indicating which node(s) to call next.\n",
        "\n",
        "Next, define a router function called `route_tools` that checks for `tool_calls` in the chatbot's output. Provide this function to the graph by calling `add_conditional_edges`, which tells the graph that whenever the `chatbot` node completes to check this function to see where to go next.\n",
        "\n",
        "The condition will route to `tools` if tool calls are present and `END` if not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_edges_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d92574a6-a42c-4c11-d8dc-66123b952ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.state:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Branch with name `tools_condition` already exists for node `chatbot`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e52bf3464156>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m graph_builder.add_conditional_edges(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"chatbot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtools_condition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36madd_conditional_edges\u001b[0;34m(self, source, path, path_map, then)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;31m# validate the condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;34mf\"Branch with name `{path.name}` already exists for node `{source}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Branch with name `tools_condition` already exists for node `chatbot`"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "# def route_tools(state: State) -> str:\n",
        "#     \"\"\"\n",
        "#     Use in the conditional_edge to route to the ToolNode if the last message\n",
        "#     has tool calls. Otherwise, route to the end.\n",
        "#     \"\"\"\n",
        "#     messages = state[\"messages\"]\n",
        "#     last_message = messages[-1]\n",
        "\n",
        "#     if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "#         return \"tools\"\n",
        "\n",
        "#     return END\n",
        "\n",
        "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
        "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"chatbot\",\n",
        "#     route_tools,\n",
        "#     {\n",
        "#         \"tools\": \"tools\",\n",
        "#         END: END\n",
        "#     },\n",
        "# )\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tools_condition_note"
      },
      "source": [
        "**Note**: You can replace this with the prebuilt `tools_condition` to be more concise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_graph_tools_markdown"
      },
      "source": [
        "## 7. Visualize the graph (optional)\n",
        "\n",
        "You can visualize the graph using the `get_graph` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_graph_tools_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "75cf526c-cb0d-4a93-b9c4-0c3ec3419f10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ask_bot_markdown"
      },
      "source": [
        "## 8. Ask the bot questions\n",
        "\n",
        "Now you can ask the chatbot questions outside its training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ask_bot_code",
        "outputId": "0ccdadbb-f0a6-4f30-cc40-e2253d67f897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:1501: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What do you know about LangGraph?\n",
            "Assistant: \n",
            "Assistant: {\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"What is LangGraph? - IBM\", \"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.942002, \"raw_content\": null}, {\"title\": \"langchain-ai/langgraph: Build resilient language agents as graphs. - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph\", \"content\": \"GitHub - langchain-ai/langgraph: Build resilient language agents as graphs. langchain-ai.github.io/langgraph/ Trusted by companies shaping the future of agents ‚Äì including Klarna, Replit, Elastic, and more ‚Äì LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials. LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. LangGraph Platform ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. langchain-ai.github.io/langgraph/\", \"score\": 0.902687, \"raw_content\": null}], \"response_time\": 1.87}\n",
            "Assistant: LangGraph, created by LangChain, is an open-source AI agent framework designed for building, deploying, and managing complex generative AI agent workflows. It utilizes graph-based architectures to model and manage the intricate relationships between various components within an AI agent workflow, offering transparency into the agent's state.\n",
            "\n",
            "Key aspects of LangGraph include:\n",
            "*   **Graph-based architecture:** It uses a graph structure where \"nodes\" represent individual components or agents in the AI workflow.\n",
            "*   **State management:** LangGraph provides low-level supporting infrastructure for long-running, stateful workflows, allowing agents to analyze past actions and feedback for enhanced decision-making.\n",
            "*   **Integration with LangChain:** While it can be used standalone, it integrates seamlessly with LangChain products, offering developers a full suite of tools for building agents.\n",
            "*   **Use cases:** It's versatile for developing AI solutions and workflows, including chatbots, state graphs, and other agent-based systems.\n",
            "\n",
            "It is trusted by companies like Klarna, Replit, and Elastic for building resilient language agents as graphs.\n"
          ]
        }
      ],
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "congrats_next_steps_tools"
      },
      "source": [
        "## Congratulations! ü•≥\n",
        "\n",
        "You've created a conversational agent in LangGraph that can use a search engine to retrieve updated information when needed. Now it can handle a wider range of user queries.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "The chatbot cannot remember past interactions on its own, which limits its ability to have coherent, multi-turn conversations. In the next part, you will add memory to address this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_memory_intro"
      },
      "source": [
        "# Add memory üß†\n",
        "\n",
        "The chatbot can now use tools to answer user questions, but it does not remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.\n",
        "\n",
        "LangGraph solves this problem through **persistent checkpointing**. If you provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off.\n",
        "\n",
        "We will see later that checkpointing is *much* more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But first, let's add checkpointing to enable multi-turn conversations.\n",
        "\n",
        "**Note**: This tutorial builds on *Add tools*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_checkpointer_markdown"
      },
      "source": [
        "## 1. Create a `MemorySaver` checkpointer\n",
        "\n",
        "Create a `MemorySaver` checkpointer. This is an in-memory checkpointer, convenient for tutorials. In production, you'd likely use `SqliteSaver` or `PostgresSaver`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_checkpointer_code"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compile_graph_memory_markdown"
      },
      "source": [
        "## 2. Compile the graph\n",
        "\n",
        "Compile the graph with the provided checkpointer, which will checkpoint the `State` as the graph works through each node:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "WUDC50fMGO0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compile_graph_memory_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "2baeec18-b59f-4349-f831-e863e60ff933"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOydB3xTVfvHT3aapGmbdC/ookAZLbai7CXIHjIURZCXIaiAiryiIggOeAXhBRFERQSRWcpGBJUihQIFCnRBaaF0t+nKanb+T5vX2n9tC0hvem7u+X7yuZ+Te25u0+SX5zzPcxbXarUiAqG14SICAQOIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgNMejMinyDVmXWqkxmk9VooEF6S+DE5vJZImeuyJntFeiEaAiL5BFtaNWmzCvq7BRNeZHe1ZMvcubA9yqVcY16Gnw+PCG7ogh+PCaQY066NriTJLiLOKSLBNEHIkQEn8D5I2VF96o9AoTBncT+YSJEZww6S3aKOvdWdf6d6h4j5e26OSM6wHQhpl9U/rq7BL6wbgPckGOhqjDCDwzM5OAp3mIp7j4Yo4V49kAph4d6jvRAjkt5sf7gxoJBk70C22Nt6ZkrxN/3lci8+F37uCIGcGhz/lPD5F6BQoQrDBXikS0FAeGiyL6MUKGNQ5vy28dIw6MxdRnZiHmcP6LwDXFilAqB0XP8rv5WoSjQIyxhnBAzr6ng+MRARwtNHoYXFgWCW2y14NgGMk6I8bGlUf2ZqEIbwZ0l5w4pEH4wS4jXzlS0j5Y6STiIqYBDknlNrVGaEGYwS4j3UjVPj5QhZtNnnHtyfCXCDAYJ8V6ahstjczhMjM/qE9henJJQhTCDQd/K3ZuaoM5iZF/efffdQ4cOoUfnmWeeyc/PRxTAF7I9/AXQAYhwgkFCLC8xhNhdiGlpaejRKSwsrKioQJTRLkqSd0eLcIIpQjToLIp8vZOEqi7XhISE2bNn9+rVa8yYMUuXLlUoaiLT6OjogoKCFStW9OvXD56q1erNmzdPnTrVdtnatWt1Op3t5QMHDty1a9fMmTPhJfHx8SNHjoSTo0ePfvvttxEFiF14pXl4JRSZIkSIE6nr+M/IyJg/f35MTMz+/fsXLVp0+/btZcuWoVp1wnHJkiVnzpyBwu7du7dt2zZlypR169bB9adOndqyZYvtDjweLy4uLjw8fOPGjT179oQL4CS06WvWrEEUIJZyNEozwgmmDIzVVJnELlT9s8nJyUKhcPr06Ww229vbu2PHjnfu3Pn7ZS+99BJYvqCgINvT69evnz9/ft68eVBmsVguLi4LFy5EdgE+CvhAEE4wRYgWC+I7UWX+IyMjoZFdsGBB9+7d+/TpExAQAC3s3y8Ds3fhwgVouMFkmkw1OpDJ/solgXyRvWBzWRCyIJxgStMMjVFVqRFRQ/v27devX+/h4bFhw4axY8fOnTsXrN3fL4NaaIvhgoMHDyYlJb3yyiv1a/l8PrIXmkoTh8tCOMEUIYqkXC2V3Qk9evQAX/DIkSPgHVZVVYF1tNm8OqxWa2xs7KRJk0CI0HzDGZVKhVoJSj3mfwZThOgk5rj7CUxGC6KAK1eugLcHBTCKI0aMgFAXRAYpmPrXGI3G6upqT09P21ODwXD27FnUSui1Fs8AAcIJBuURoYs5+6YGUQA0xBAsHzhwAJJ/KSkpEB2DIn18fAQCASgvMTERGmKIY9q2bXv48OG8vLzKysrly5eDZ6lUKjWaRt4SXAlHCKvhbogCbl9VebXBa5Asg4QY1El8N4USIUI4DA3u6tWroTtk1qxZYrEYfEEut6btg1D68uXLYCPBHH766acQXI8fPx6SiE8++eTrr78OTwcNGgS5xgY39Pf3h1QiJB3BrUQUcC9NGxRh79x+8zBohLZBbzn2XeHYuX6I2dy/pc2+qe433hPhBIMsIl/A9vQXXP2Nwq4zWnD+sCLiaReEGcxa6aHHCPnGhVlNzRy1WCwDBgxotApiC8gCQtr571XBwcFbt25F1ACpcgjA0SO+pXbt2tX12TQAvEM3L76HH16RCmLg5KnrZystFmtUv8a12FRKRa/XQ+TRaBVIQSKhcE2Ff/CWIDACP7XRqmPfFfQe6yGV8RBmMHEW3/GtheHRzvRakaNFwPkfZ+Io0WHTfS4cLSvJ1SEmER9bKvfhY/vzY+i85pp+jv/mPTVcTveVbh4SUKFnoKBDjBThCkPHzYNjN35BwOVfKlITsRs037LAT+7QpnypjIuzChFZhOnCMcXdVC1E02074pXgbRGSTpWnJir7T/QMDMfd8JNl6VBZgf780TKBE9svzAn6G0TOtE9plebpc9I1V36t6NLbtftQGZuN10CbRiFC/B/5WdW3LqvupmrcvHgyL77YhSuWcsUuHDNeA5kbh8WyqspNGqXZarHevqoWitmhXSWgQtwGHTYDEWJDiu5Vl+YbNFXwvZrAlmhVLalE6HHOzs6OiIhALYrEjYusNWMund24viFOzm7YpQkfCBGiXcnKylq8ePHevXsR4f9DFnMnYAERIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEAhYQIRKwgAiRgAVEiHaFxWLV7XBBqA8Rol2xWq0lJSWI8DeIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEAhYQIRKwgAiRgAVEiAQsIEIkYAHZ8McePP/881qtFgoGg6GsrMzHxwfVbkF/8uRJRKiFodvk2pnRo0cXFRUVFBQoFAr45RfU4uzsjAh/QoRoD8AiBgYG1j/DYrF69eqFCH9ChGgPQHbjxo3jcDh1Z9q0aTNp0iRE+BMiRDsxceLEgIAAWxl02bdvX5unSLBBhGgnuFwuNNACgQDK/v7+48ePR4R6ECHaD2idQYJQ6NGjBzGHDWB6HtFosFQUGdRKO+1TP3LgjFOWU/2enJSdokHUw2YjN0++izsN9hFndB4x8XhZ5jU1T8B2lvHMRgf8HCSu3NzbGhBitwFugeEihDHMFWJ8bCmLxY4aKEeOjlFvObUjv9douV8ovlpkqI+YcFjB5jBChQCY/GEzAs7sV5Tm6xGuMFGIqkpjcY4usj8jVFjH0yM9rpyuQLjCxGClvNDA4jDuF+jizr+foUW4wkSLqKwwybwEiGHwhRxnOU+ntVN+4FFhZPrGUpO1QcxDVW6ETh2EJWQ8IgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIyZ+WxmDBp6LffbUSPwdJli95eOAcxHiLEViDu4N7PVi1Fj8Hdu1nPTx6BHAjSNLcCt26locfj1u3HvQNuECE+FGazed/+nT9s3wLljh06T5s6u3PnSFsVl8s7ELdn89fr+Hx+p06Ri99d7iJ1QbVG6/CR/VevXS4qKmjbJnjYsDGjR9XMZV7w1qzr169C4Zdfjn29+UdUO98+6crFPXu2p6ReDwlpN++NRe3C2ttunpAQD3805/5dFxfX0NDw+W/828vL+/ttm7fv+BZq+w+MPnHsnFAoRPSHNM0PxZZvNhw6tG/5R6s/eO8TDw+vfy9+4/79e7aq+LOnNRr1qpUb3ln4YUpK8vffb7Kd3/jVmsuXL8yf9++Vn60HFf53/arEiwlwft0XWzp06DR48PDff02yCQ50dvDQ3smTX/n0k3UWi+WDJW/ZZrSBOj9c9g5cuXf38aVLVhYXF65bvxLOvzLt1ecnvQyKhDs4hgoRsYgPg0qt2rvvxwXz342Jfgqedu/eU6vVlJUrAgPbwlORSDzlpX/Zrkw4H3/j5jVbecmSz+AyH29fKEdFRv/88+FLl88/1b3n3+9fUVG+YN677u4eUH55yszF780HkxkZ+cTW7zf16T1g/HOT4TxYxLlz3lr4ztyMW2ntwzsih4MI8cHk1hq/9u0jbE+5XO7yjz6vq+3cKbKu7CJ1Nej/nClntR44sPvipYTc3BzbCR8fv0bvHxIcZlMh0CmiKxwLCvNAiNnZmX37DKy7LLxdjf4yMlKJEBmKWqOGo1DQeCMIuqwr1w3Ehxb23ffmG42GmTNej4yMdpY4vzH/X03dXyyW1JVFopqpx0pllVqt1uv1gnp/1FYFVhY5IsRHfDBikRg9ogJuZ2aA6Zrz6pu9e/UHFcIZtVrV1MXVuuq6sk30UqmLzfnT1avS1L4BucwdOSJEiA+mbdsQMHvXb1y1PYVIAqzdyZNHm3lJVVUlHD3cPW1P793LhkdTF9+/f1en09nKtsyOv18g/MXwdh1SU2/UXWYrB4eEIUeECPHBiMXiZwYNg6j5xM+HryUnbfjy8ytXLkLk28xLIF8DStqzd4dSpYT4Gl4CgU5RcaGt1s8vID09BTI7EKbAU6HQafWaFXBlZWXFzp+2enp62XJDY8dMOpdwJjZ2F1TB3/1q0xfdomLCQsNRzcJ2gWVlinPnzkBeCTkERIgPBWRhwNVb88Unb7396s2bycuXfW4LmZsCcivvv/dxWvrN0WMGvPfBmzP+9dqoUeNBfFNfqUkljhw+DrzJdxa9lpWdaTQZIUAJDAyaMPFZ6DAEYX284gubrwmJm39Nn7tn3w64yar/LOvSOerDJZ/Z7v9U914QJC1ZutBgMCCHgImLMN08V1Wca+g+zAMxjF2rsqcuaStwwtH6kKiZgAVEiAQsIEIkYAERIgELiBAJWECESMACIkQCFhAhErCACJGABUSIBCwgQiRgAREiAQuIEAlYwEQh8vhsgZCJ49/kPgI2B+EJE78PmQ8v7w6+W99QRFWZQas0wY8QYQkThegZIOQLWPpqBxnb/JCU3K8OjZIgXGHoCO1eY9xP7yxAjKEgW5txserpYfhuP8jcbXLLCvX71+VFP+vh4s6TuPAc8mNgsVB5kV5Vbsi6rnr+nQA2G9NtpxDDNw436CyXfylLv1bMYQnZVnvEbRar1Wg0Cvh8RA0arZbFYnE4HHYt7n5C0GJguKhrH1eEN4xO33B4VvfwcnNhwozZs5FdyMrKWrz4g7179yJqWLx48cmTJ0GLbm5uEolEkCHw9fVtZ2rXtQ/uSzAy1yJu3759+PDhYrHYnusYqVSqK1eu9OvXD1FDRkbGggULFApF/ZMWi8XHx+fYsWMIYxgarMTGxlZUVMjlcjuvpuXs7EydClHNAj3tO3To0OAk/NgwVyFioBB/++03OPbs2XP+/PnI7pSWln711VeISiZPngztct1T8BT/+OMPhD3MEuLKlSuzs2uW/vD29katgVKpPHPmDKKSmJiYkJAQm8cFjXJwcPChQ4cQ9nCWLVuGGMCdO3dkMhk0UuAXotaDx+P5+/u3bdsWUYlIJLp06ZJer4e/BU4IxEYJCQm9e/dGGMOIYAViyYEDBw4aNAgxhhdffLG4uPj06dO2pyDHuLi4H3/8EeGKgwtRrVZXVlampaUNHjwYYQD4iPv27Zs7dy6yO+np6VOmTPnhhx8iIiIQfjiyj7hixQpIZEDzhIkKkV18xKaAaDopKWnVqlX79+9H+OGwQoTGqHPnzlR7Y4+KIBh+DQAADxRJREFUp6dnq5jDOiB7mpmZ+dFHHyHMcMCmecuWLbNmzTIYDHzKetLozuHDh3fu3Lljxw58PiJHs4gffvihq2tNvyqeKrRDHvFhGDVq1CeffNK3b9/k5GSEB44jxPj4eDjOmzdv4sSJCFda0UdsQGho6IULFzZs2PDTTz8hDHAQIUK2wrbKqrs71mudt7qP2IDvvvuusLDwgw8+QK0N7X3EvLw8+HahvwS6WRHhH3HixIlvvvkGXEZI+KNWgsYW0WQyzZw5U6fTgTtIFxVi4iM2YOjQoWvXroXj5cuXUStBVyGCIYduqzlz5oCvg+gDPj5iA9q0aXP27FloqSHjjVoD+gkROvLffPNNECIEfd26dUO0AjcfsQGbN2+uqqpatGgRsjv08xGXLl0KHcd9+vRBBGr49ddf161bBy6jLRFmH+gkRGg1pk6diuhMK/Y1PxIFBQXQMb18+fKePXsiu0CbpvnZZ5/t1KkTojnY+ogN8PX1Bbu4Z8+eb7/9FtkFGljEq1evgi8I0bEDbJJN9ZyVFmfTpk23b9+GmBpRDNYWUaPRDBkyRCqVopoN6xxhq3aq56y0OJCXGDt2LHwLJSUliErwtYhqtRqS/m5ubph3ljwSdPERG6BQKMBlXLlyZdeuXRE1YGoRDxw4AC1yWFiYI6kQ1dr1a9euIboB3wL0vmzcuDE/Px9RA6YT7DMzM41GI3I4oGmGnpXq6mroGaedswGmAYIYRA2YWsRXX311xIgRyBHh8XhOTk4QkILjgehDRkZGeHi4bWQJFWAqRBcXl1bsgLcDkBBdsGABog/p6el/n7rfgmAqxK+//vro0aPIoQGjCMfc3FxEB9LS0jp27IgoA1MhQo8n5G4QA4iPj4fMIsIeqi0ipukbECKXy3Xs1rmOjz/+GIehqc0THR2dlJSEKIP4iK2PTYWJiYkIV6BdptQcIuIj4kNeXt7JkycRllDdLiPiI+LD+PHjlUolwhKqIxWErRBnz57tqHnEZpgwYQIcd+3ahTCDuRaRUT5iA+RyOVarglgsFujogmw2ohLiI2LH4MGDsVopxQ7tMiI+Ip5ArgTVrlqBMMAO7TIiPiLOjB07dufOnai1sY8QMR19Az4iYjxRUVFeXl6otYGm+YUXXkAUQ3xErLENuwLTiFoJk8l09+7dsLAwRDHER6QBmzdv3rFjR/0zdlt61D6RCiJ9zXTBUAuHw3Fycho2bFhxcfGQIUM+/fRTRDF79uzJycmxw5R74iPSA34tvXr1cnV1LSkpYbFYqamp5eXlMpkMUQlYxJiYGEQ9xEekE5DrLioqspVBhXbYycc+ITMiPiKNeO655+rPXYLP59SpU4hKwBnIzc0NCQlB1INp0wx5RPAREeFPIHAGXw3VbmlmOwMFOJOdnR0cHIyowW6RCiJ9zXQhLi4OtAhdf7aFkaD/F44QslDaOtutXUbYWkTwEf38/EjnSn2WLFkCxxs3bvxRS1lZWVWFNv7XS+NGvYio4VbqfUiqqypM6J8CKRmp7KE0hlf6ZsCAAeAd1r0liA2h7O3tffz4cUSoR9Kp8hvnKiwsk0lvdaJsfjRkszlc7uNMIHXzEeRnakO7irsPk0tlvGauxMsi9ujRAzRX5wahWk9o5MiRiFCPn38oksh4Q6cHSlx5CHtMRktliWHff/PGvebn5tnkniN4+YjQp9lgLQF/f387dHTSiBPbity8BV37yGmhQoDLY7v7CSe+FRS3MV9Z3uTqHXgJMSIiov4iiNA0P/vss/ZctxRz7qVp+E6cjk+5IRrSf5JP4vHypmqxi5pffvnluoWXwBzivHuP/SnJ1fMEdF1/381LcCdZ1VQtdv8VJK66dOliKw8dOtTNjZa/forQa83uPgJETzhcVmC4uLLU0Ggtjj+vadOmQV8WBMvEHDZAozSb6LxGWnmxoallnB43ai7I0lYpTBqVSas0W8wQ8FtQCyDvFT4HEtpJJ/SQtUWPjcCJzUIskZQDD7mvwMOXrkbFgfmHQsxJ19y+qs5O0bh5O1mtLA6Pw4YHh9NSWclOXfrBUdVCvc1qLctiNpvzTWaDzqirMurMIV3E7aOdvdo4wnLIjsEjC7HwbvXZuDKeiM/iCkKeduPyOIhuGKpNZQpN/MEKJxHqPUbu6kG2dW59Hk2Ip3eVFmTr5EEysRuNbQnfiSsLqBnvqCzRxG4o6PCkc48RckRoVR42WIH8+LblOTqzILCbL61VWB+ppzjk6YCSIjbkWhGhVXkoIZpN1i2Ls306eknkDjgixtVPynOR7l5NjwUzHZUHC9FisW5alNVxYJBATI8+pX+ARC6S+sl++DgHEVqJBwtx52f3w3r4IUdH5CqUBbge+45OC6w7Eg8Q4plYhWuAq0DMiLjS2VNiRILk+EpEsDvNCbGsQH83RePsIUGMwdXX5dxBBe22DnYAmhPi2YNl7kHUzlbEEO92bn8cLEME+9KkEIvuVZvMbGcPEcKS5JunFy7prtZUoJbGva1rfrZeX21GhFrGjBu0fQflm+U2KcQ71zXQc4eYCYt9L1WLHIKPlr97/MQhhD1NCjHrhsbZE1NzSDUimTgzWY0cglu30hAdaLyLr6LE4OTMoy5Yvnf/xi+/f5ublyYRu3UI7zW4/wyhsCZVnpC471T81jnTN23fvbi4JNvHK7RPjxdiuv1vLt/RnzckXT8u4IuiugzxdA9ElCH1FBWmYrqu+iPRf2DNgp+fr16xafPaI4fOQDkhIf6H7Vty7t91cXENDQ2f/8a/vby8bRc3U1VH4sWEPXu2Z9xKlcncO3XqOmvGG3J5y2wf27hFVFeadNUtMqCrERRluV9ve8No1L8+69upk1cVFmdu2jrHbK6Zs8jh8qqrVQePrZ445r3Plyd26TRg78GPKyprFtk4fyn2/KX944a/M3/293I331O/f4cog8ViqSuMGuU/n0aJCT8fT4DjOwuX2FSYdOXih8veGTx4+N7dx5cuWVlcXLhu/Urblc1U1XE7M2Pxe/OjomK2bd0/741FWVm3V/1nGWohGheiVmnmUDas5ur1n7kc3rQXVnl5tPX2DJ4w+v38wlsp6fG2WrPZ+Ez/GW0COoMaoiOHQyYlv/A2nD93YW+XiIEgTZFICjYyNDgaUQlfyNFU0V6IDdj6/aY+vQeMf24y2LyIiC5z57yVmHguo7btbqaqjpSbyUKh8KUXp4Ol7P5kjzWfb3rhhWmohWhCiCoTh0/VTFNolwP8O4rF/5sSJXPzkcv87+Yk110Q6BdhK4icpHCs1qlAjoryXC/PoLpr/H3bIyrhOXG09LeIDcjOzmzfPqLuaXi7muVEMjJSm6+qo1PnSJ1Ot/j9Bfv278zLzwXJRkW2mDloUm0sRFVSt1qnzs1Pg+RL/ZNK1V+pu7+PJtfpNRaLWSD4K3ji850QlVjMNe8DORBqtVqv1wsEf42cEolqPk+tVtNMVf07tAtrv/Kz9WfP/rrlmw1fbVr7RLcnp02dDZ4iagkaF6JIyjUbdYganJ3lQW0ihwyYVf+kWNzcgohCgZjN5hjrvSW9gdr0itlgFksdahUoYe2CEDpddd0ZTa3O5DL3Zqoa3ARaZHi8Mu3VK1cuxh7Y9d77C+IOnOZwWsCLa7xpFjlzzEaqMrq+XmGVVUXBbaNCg5+wPSQSN0/3ts28BGykm6vPvfs3686k30pAVGLQmUVS+g0+bwYulxverkNq6o26M7ZycEhYM1X175CcfOXipfNQcHf3GDJkxGtz31apVQpFKWoJGheiVMbl8alqmCAjY7FYDp9YazDoSkpzjp78cs2XkwuL7zT/qq6dBt1M+x06VKD82x/bc/JSEGVYLFaJK9cBLKJAIPDw8ExKSryWnGQymcaOmXQu4Uxs7C6lSglnvtr0RbeomLDQmi2lmqmqIyX1+rKPFh05eqCysiItPeVA3G5QJDxQS9D4Z+3izjfpzDqVQejc8qlECHsXvv7T73/sWLd5aknpvUD/iAlj3n9g8DGo7ysaTcXB42t+3Ps+tOyjhi74ad+HFI1OUBZr3DwdpFfpxcnTv9+2+dLl87t+OgrZmVJFyZ59O778ag1EvtFPPDVzxuu2y5qpqmPihJdAgl9uXP3F2k/5fP6A/kPWfrGlRdpl1MxqYBeOleXds3oEM3F+e0FqScxASViUM8KMn38o8g2RBHWm63iouA05o1/1dXFv5EfeZBdfaFex1eRo+YuHhMUyB0WQZULtSpNukIe/0ElkrSrWuHg1/pVUVpWs/rLxdbqcBJJqfeN9td4ewa/P+ga1HB98MrCpKuit4XAa+QfBGZg1dX1TryrNrgjq6MTl03WJGZrSnD/eZ5z7/nX5TQnRWSJ7a+6ORqsgCuHzG5/px2a3cATQ1HuoeRtGPZ/XyKIOXG6Tjq/FbCm9WzXhNXssX06oT3OycJHzOnSXlJWqnD0a8ZbA2MjcfFFr07LvQVlY1W9Cy/TiEx6JBzRAPUa4axVqbSVVyW2sqCpUSsSWjt3JXkOtwIM9oUlv+d+/VmTUOXjgUlmkri5XD5rsiQitwUO55LNXBWcm5DqwXawqUiOd5vmFAYjQSjyUEKGHbe7qUGV+ubJYhRyOitwKPqt6zJzW93eZzCMkKcBgyOXm7MQ8ZYmDbE5Wka/MOJMTFM4dOs0bEVqVR0um9Bwp79jd+WxcmSJLa+XwpB5iOq5DUq3Uq0q1Fr3e3Zc3bFkbgZNDDW6gKY+c1XPz5I+e7VN0T5eZrM66USwQcS0WFofPqVmrkwvfKI5T08G1MBnNFoPJZDAbqo0CJ3ZYpKRdNw+yMiI+/MP0sndbITx6j3EvLzJUKWqmd2iqTGaTxWzCUYh8IYvNYYulIpGU4+7Hl7gwdZosxjxuP4fMmw8PRCA8HmQrWjohduHSetEDmbegKeeNdO3TCScxW5GvR/TEaLDk3da4uDfefhIh0gmvNkKjnq6L8pQX6ZsZ4kmESCcC2olYLHTtN1ouVvbbTwU9RzW5aD5e+zUTHoazB0qNRmtIF6nclwar6kNGpapU//vuoinvB4qbzlcQIdKSlAtVqeeVOq1ZT9nKMC2Ch5+gssQQ1Fncc6R789tZEiHSGPjqDDqshWi1WIXih+q4IkIkYAHJIxKwgAiRgAVEiAQsIEIkYAERIgELiBAJWPB/AAAA///xDrdZAAAABklEQVQDAF1BImL6Ux2yAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interact_chatbot_markdown"
      },
      "source": [
        "## 3. Interact with your chatbot\n",
        "\n",
        "Now you can interact with your bot!\n",
        "\n",
        "* Pick a thread to use as the key for this conversation.\n",
        "* Call your chatbot.\n",
        "\n",
        "**Note**: The `config` must be provided as the **second positional argument** when calling `stream()` or `invoke()`. It should *not* be nested within the graph inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "interact_chatbot_code",
        "outputId": "4d6a2091-5a7c-4f94-de1a-0430cc341f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Will.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you, Will! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "user_input = \"Hi there! My name is Will.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=user_input)]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "follow_up_markdown"
      },
      "source": [
        "## 4. Ask a follow up question\n",
        "\n",
        "Ask a follow up question. Notice we use the *same* `config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "follow_up_code",
        "outputId": "a00ab02f-184d-4959-90ba-0c491f06a9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Will.\n"
          ]
        }
      ],
      "source": [
        "user_input = \"What is my name?\"\n",
        "\n",
        "# The config is the **second positional argum¬´ent** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=user_input)]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "\n",
        "# print(f\"User: {user_input}\")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wojzOdHL1EnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unique-cell-id-test1"
      },
      "source": [
        "Notice that we aren't using an external list for memory: it's all handled by the checkpointer!\n",
        "\n",
        "Don't believe me? Try this using a *different* config (a new `thread_id`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "follow_up_new_thread_code",
        "outputId": "f67b76c6-11de-45eb-a466-dee412f45e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I do not know your name. I am a large language model, trained by Google.\n"
          ]
        }
      ],
      "source": [
        "# The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7s1IenBfOzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unique-cell-id-test2"
      },
      "source": [
        "Notice that the only change we've made is to modify the `thread_id` in the config. The bot doesn't remember the name in this new thread."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspect_state_markdown"
      },
      "source": [
        "## 5. Inspect the state\n",
        "\n",
        "By now, we have made a few checkpoints across two different threads. But what goes into a checkpoint? To inspect a graph's `state` for a given config at any time, call `get_state(config)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inspect_state_code",
        "outputId": "0a3f1c4e-d1e3-44e2-dedf-14fd79570bdb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snapshot for thread_id='1':\n",
            "Next node to execute: ()\n",
            "--- Messages ---\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Will.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you, Will! How can I help you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Will.\n"
          ]
        }
      ],
      "source": [
        "snapshot = graph.get_state(config)\n",
        "\n",
        "print(\"Snapshot for thread_id='1':\")\n",
        "print(f\"Next node to execute: {snapshot.next}\")\n",
        "print(\"--- Messages ---\")\n",
        "for msg in snapshot.values['messages']:\n",
        "  msg.pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "congrats_next_steps_memory"
      },
      "source": [
        "The snapshot above contains the current state values, corresponding config, and the `next` node to process. In our case, the graph has reached an `END` state, so `next` is empty.\n",
        "\n",
        "## Congratulations! ‚úÖ\n",
        "\n",
        "Your chatbot can now maintain conversation state across sessions thanks to LangGraph's checkpointing system. This opens up exciting possibilities for more natural, contextual interactions. LangGraph's checkpointing even handles arbitrarily complex graph states, which is much more expressive and powerful than simple chat memory.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "In the next tutorial, you will add human-in-the-loop to the chatbot to handle situations where it may need guidance or verification before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add human-in-the-loop controls üßë‚Äçüíª\n",
        "\n",
        "Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.\n",
        "\n",
        "LangGraph's **persistence** layer supports **human-in-the-loop** workflows, allowing execution to pause and resume based on user feedback. The primary interface to this functionality is the `interrupt` function. Calling `interrupt` inside a node will pause execution. Execution can be resumed, together with new input from a human, by passing in a `Command`. `interrupt` is ergonomically similar to Python's built-in `input()`, with some caveats.\n",
        "\n",
        "**Note**: This tutorial builds on *Add memory*."
      ],
      "metadata": {
        "id": "human_loop_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Add the `human_assistance` tool\n",
        "\n",
        "Starting with the existing code, add the `human_assistance` tool to the chatbot. This tool uses `interrupt` to receive information from a human.\n",
        "\n",
        "First, set up the LLM and the necessary imports. Then, define the `human_assistance` tool and add it to our list."
      ],
      "metadata": {
        "id": "add_human_tool_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "@tool\n",
        "def human_assistance(query: str) -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt({\"query\": query})\n",
        "    return human_response[\"data\"]\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool, human_assistance]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    # Because we will be interrupting during tool execution,\n",
        "    # we disable parallel tool calling to avoid repeating any\n",
        "    # tool invocations when we resume.\n",
        "    assert len(message.tool_calls) <= 1\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")"
      ],
      "metadata": {
        "id": "add_human_tool_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354f783d-8b2f-4a64-b8e2-0c98a6e03f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b7db3d1af50>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tip**: For more information and examples of human-in-the-loop workflows, see [Human-in-the-loop docs](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/). This includes how to review and edit tool calls before they are executed."
      ],
      "metadata": {
        "id": "human_in_loop_tip_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Compile the graph\n",
        "\n",
        "We compile the graph with a checkpointer, as before:"
      ],
      "metadata": {
        "id": "compile_human_graph_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "compile_human_graph_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Visualize the graph (optional)\n",
        "\n",
        "Visualizing the graph, you get the same layout as before ‚Äì just with the added tool!"
      ],
      "metadata": {
        "id": "visualize_human_graph_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "visualize_human_graph_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c2a2807a-202b-4ed5-cdc6-5accdce744f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prompt the chatbot\n",
        "\n",
        "Now, prompt the chatbot with a question that will engage the new `human_assistance` tool. Execution will pause when the tool is called."
      ],
      "metadata": {
        "id": "prompt_human_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "prompt_human_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f66031c-cd7c-49e6-9b3c-e430348d1b41",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (e4b0660e-e655-4ae5-a1e2-bb3dc3ee493c)\n",
            " Call ID: e4b0660e-e655-4ae5-a1e2-bb3dc3ee493c\n",
            "  Args:\n",
            "    query: Expert guidance for building an AI agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chatbot generated a tool call, but then execution has been interrupted. If you inspect the graph state, you see that it stopped at the tools node:"
      ],
      "metadata": {
        "id": "inspect_human_state_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot = graph.get_state(config)\n",
        "print(snapshot.next)"
      ],
      "metadata": {
        "id": "inspect_human_state_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798ee7c0-6520-44f2-ad5c-b8ebd7d0d106",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('tools',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Resume execution\n",
        "\n",
        "To resume execution, pass a `Command` object containing data expected by the tool. For our `human_assistance` tool, we expect a dictionary with a key `\"data\"`."
      ],
      "metadata": {
        "id": "resume_human_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_response = (\n",
        "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
        "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
        ")\n",
        "\n",
        "human_command = Command(resume={\"data\": human_response})\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "resume_human_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb49a04-2c5a-4eb6-9b6f-0af77770c21e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance (e4b0660e-e655-4ae5-a1e2-bb3dc3ee493c)\n",
            " Call ID: e4b0660e-e655-4ae5-a1e2-bb3dc3ee493c\n",
            "  Args:\n",
            "    query: Expert guidance for building an AI agent.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance\n",
            "\n",
            "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input has been received and processed as a tool message. The graph continued execution and provided a final answer.\n",
        "\n",
        "## Congratulations! üôå\n",
        "\n",
        "You've used an `interrupt` to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since you have already added a `checkpointer`, as long as the underlying persistence layer is running, the graph can be paused **indefinitely** and resumed at any time as if nothing had happened.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "So far, the tutorial examples have relied on a simple state with one entry: a list of messages. You can go far with this simple state, but if you want to define complex behavior without relying on the message list, you can add additional fields to the state."
      ],
      "metadata": {
        "id": "congrats_next_steps_human"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkxJfrn8L0o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customize state ‚öôÔ∏è\n",
        "\n",
        "In this tutorial, you will add additional fields to the state to define complex behavior without relying solely on the message list. The chatbot will use its search tool to find specific information and then forward it to a human for review, updating the custom state fields in the process.\n",
        "\n",
        "**Note**: This tutorial builds on *Add human-in-the-loop controls*."
      ],
      "metadata": {
        "id": "cust_state_md_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Add keys to the state\n",
        "\n",
        "Update the chatbot to research the birthday of an entity by adding `name` and `birthday` keys to the state:\n",
        "\n",
        "**API Reference:** [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#add_messages)"
      ],
      "metadata": {
        "id": "cust_state_md_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    name: str\n",
        "    birthday: str"
      ],
      "metadata": {
        "id": "cust_state_code_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding this information to the state makes it easily accessible by other graph nodes and the graph's persistence layer.\n",
        "\n",
        "## 2. Update the state inside the tool\n",
        "\n",
        "Now, populate these state keys inside of the `human_assistance` tool. This allows a human to review the information before it is stored in the state. We will use `Command` to issue a state update from inside the tool.\n",
        "\n",
        "We also need `InjectedToolCallId` to associate the `ToolMessage` we'll create with the original LLM tool call."
      ],
      "metadata": {
        "id": "cust_state_md_3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GuGovRmdtNKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import InjectedToolCallId, tool\n",
        "\n",
        "from langgraph.types import Command, interrupt\n",
        "\n",
        "@tool\n",
        "# Note that because we are generating a ToolMessage for a state update, we\n",
        "# generally require the ID of the corresponding tool call. We can use\n",
        "# LangChain's InjectedToolCallId to signal that this argument should not\n",
        "# be revealed to the model in the tool's schema.\n",
        "def human_assistance_name_birthday(\n",
        "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
        ") -> str:\n",
        "    \"\"\"Request assistance from a human.\"\"\"\n",
        "    human_response = interrupt(\n",
        "        {\n",
        "            \"question\": \"Is this correct?\",\n",
        "            \"name\": name,\n",
        "            \"birthday\": birthday,\n",
        "        },\n",
        "    )\n",
        "    # If the information is correct, update the state as-is.\n",
        "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
        "        verified_name = name\n",
        "        verified_birthday = birthday\n",
        "        response = \"Correct\"\n",
        "    # Otherwise, receive information from the human reviewer.\n",
        "    else:\n",
        "        verified_name = human_response.get(\"name\", name)\n",
        "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
        "        response = f\"Made a correction: {human_response}\"\n",
        "\n",
        "    # This time we explicitly update the state with a ToolMessage inside\n",
        "    # the tool.\n",
        "    state_update = {\n",
        "        \"name\": verified_name,\n",
        "        \"birthday\": verified_birthday,\n",
        "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
        "    }\n",
        "    # We return a Command object in the tool to update our state.\n",
        "    return Command(update=state_update)"
      ],
      "metadata": {
        "id": "cust_state_code_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prompt the chatbot\n",
        "\n",
        "Prompt the chatbot to look up the \"birthday\" of the LangGraph library and direct it to use the `human_assistance` tool once it has an answer. This will trigger the search, followed by the interrupt."
      ],
      "metadata": {
        "id": "cust_state_md_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool, human_assistance_name_birthday]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    message = llm_with_tools.invoke(state[\"messages\"])\n",
        "    assert(len(message.tool_calls) <= 1)\n",
        "    return {\"messages\": [message]}\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "user_input = (\n",
        "    \"Can you look up when LangGraph was released? \"\n",
        "    \"When you have the answer, provide the name which should be LangGraph and the birthday and use the human_assistance_name_birthday tool for review.\"\n",
        "    \"If you cannot find the birthday and name, you just use random ones\"\n",
        "    \"once you have the correction from humans, you give the users the correct answers without any explanations\"\n",
        ")\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "cust_state_code_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2b6ea0-3e12-49f7-f68e-ea3be51e249e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Can you look up when LangGraph was released? When you have the answer, provide the name which should be LangGraph and the birthday and use the human_assistance_name_birthday tool for review.If you cannot find the birthday and name, you just use random onesonce you have the correction from humans, you give the users the correct answers without any explanations\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (875549b2-97ed-4d17-a5de-1374b2359c36)\n",
            " Call ID: 875549b2-97ed-4d17-a5de-1374b2359c36\n",
            "  Args:\n",
            "    query: LangGraph release date\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"LangGraph release date\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale ...\", \"url\": \"https://blog.langchain.dev/langgraph-cloud/\", \"content\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably Our new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. Separate from the langchain package, LangGraph‚Äôs core design philosophy is to help developers add better precision and control into agentic workflows, suitable for the complexity of real-world systems. LangGraph Cloud, currently in closed beta, is infrastructure for deploying your LangGraph agents in a scalable, fault tolerant way. LangGraph Cloud: Scalable agent deployment with integrated monitoring To complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, now available in beta, which provides infrastructure purpose-built for deploying agents at scale. LangGraph Cloud also brings a more integrated experience for collaborating on, deploying, and monitoring your agentic app.\", \"score\": 0.30869067, \"raw_content\": null}, {\"title\": \"Releases ¬∑ langchain-ai/langgraph - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph/releases\", \"content\": \"Releases ¬∑ langchain-ai/langgraph ¬∑ GitHub GitHub Copilot Write better code with AI GitHub Advanced Security Find and fix vulnerabilities *   Why GitHub *   GitHub Advanced Security Enterprise-grade security features Search code, repositories, users, issues, pull requests... github-actions Add tests for stream_events when using imperative api github-actions Add message state test prebuilts hitl: fix branching logic + add structural snapshot tests (#4767) sqlite: Add test for search with list filters (#4747) github-actions Changes since prebuilt==0.2.0 github-actions Changes since prebuilt==0.1.8 prebuilts hitl: fix branching logic + add structural snapshot tests (#4767) sqlite: Add test for search with list filters (#4747) fixing prebuilt tests github-actions github-actions github-actions sqlite: Add test for search with list filters (#4747) github-actions github-actions github-actions Add clear cache methods\", \"score\": 0.2966802, \"raw_content\": null}], \"response_time\": 1.64}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (c7398dcd-2c78-4ed3-ae52-2ec304e31a5d)\n",
            " Call ID: c7398dcd-2c78-4ed3-ae52-2ec304e31a5d\n",
            "  Args:\n",
            "    query: LangGraph v0.1 release date\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"LangGraph v0.1 release date\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale ...\", \"url\": \"https://blog.langchain.dev/langgraph-cloud/\", \"content\": \"Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably Our new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. Separate from the langchain package, LangGraph‚Äôs core design philosophy is to help developers add better precision and control into agentic workflows, suitable for the complexity of real-world systems. LangGraph Cloud, currently in closed beta, is infrastructure for deploying your LangGraph agents in a scalable, fault tolerant way. LangGraph Cloud: Scalable agent deployment with integrated monitoring To complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, now available in beta, which provides infrastructure purpose-built for deploying agents at scale. LangGraph Cloud also brings a more integrated experience for collaborating on, deploying, and monitoring your agentic app.\", \"score\": 0.98563, \"raw_content\": null}, {\"title\": \"Releases ¬∑ langchain-ai/langgraph - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph/releases\", \"content\": \"Releases ¬∑ langchain-ai/langgraph ¬∑ GitHub GitHub Copilot Write better code with AI GitHub Advanced Security Find and fix vulnerabilities *   Why GitHub *   GitHub Advanced Security Enterprise-grade security features Search code, repositories, users, issues, pull requests... github-actions Add tests for stream_events when using imperative api github-actions Add message state test prebuilts hitl: fix branching logic + add structural snapshot tests (#4767) sqlite: Add test for search with list filters (#4747) github-actions Changes since prebuilt==0.2.0 github-actions Changes since prebuilt==0.1.8 prebuilts hitl: fix branching logic + add structural snapshot tests (#4767) sqlite: Add test for search with list filters (#4747) fixing prebuilt tests github-actions github-actions github-actions sqlite: Add test for search with list filters (#4747) github-actions github-actions github-actions Add clear cache methods\", \"score\": 0.98219, \"raw_content\": null}], \"response_time\": 5.25}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance_name_birthday (7ff3081c-75f3-4cb1-b522-beb351578e75)\n",
            " Call ID: 7ff3081c-75f3-4cb1-b522-beb351578e75\n",
            "  Args:\n",
            "    birthday: January 1, 2023\n",
            "    name: LangGraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've hit the `interrupt` in the `human_assistance` tool again. The chatbot *thinks* it found 'Jan 17, 2024' but asked for verification.\n",
        "\n",
        "## 4. Add human assistance\n",
        "\n",
        "The chatbot failed to identify the correct date (or rather, it *did*, but we're testing the flow), so supply it with information (or confirm it). Let's pretend it was wrong and correct it."
      ],
      "metadata": {
        "id": "cust_state_md_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_command = Command(\n",
        "    resume={\n",
        "        \"name\": \"LangGraph\",\n",
        "        \"birthday\": \"Jan 17, 2024\",\n",
        "    }\n",
        ")\n",
        "\n",
        "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "cust_state_code_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6083ce-a6c9-495e-cf11-38bb4039b744",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  human_assistance_name_birthday (7ff3081c-75f3-4cb1-b522-beb351578e75)\n",
            " Call ID: 7ff3081c-75f3-4cb1-b522-beb351578e75\n",
            "  Args:\n",
            "    birthday: January 1, 2023\n",
            "    name: LangGraph\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: human_assistance_name_birthday\n",
            "\n",
            "Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph was released on Jan 17, 2024.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that these fields are now reflected in the state:"
      ],
      "metadata": {
        "id": "cust_state_md_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot = graph.get_state(config)\n",
        "\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})"
      ],
      "metadata": {
        "id": "cust_state_code_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276f47ab-f575-4ba3-91b2-bd626dceba25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Manually update the state\n",
        "\n",
        "LangGraph gives a high degree of control over the application state. For instance, at any point (including when interrupted), you can manually override a key using `graph.update_state`:"
      ],
      "metadata": {
        "id": "cust_state_md_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Updating state manually...\")\n",
        "new_config = graph.update_state(config, {\"name\": \"LangGraph (library)\"})\n",
        "print(f\"State updated. New config: {new_config}\")"
      ],
      "metadata": {
        "id": "cust_state_code_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67e0685-c9f2-46d5-c948-8138c95aed3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating state manually...\n",
            "State updated. New config: {'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03b278-eeb1-666f-8008-b5a8d06db5f4'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLkplrSD-eWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. View the new value\n",
        "\n",
        "If you call `graph.get_state` (using the *original* config, as `update_state` creates a *new* checkpoint), you can see the new value is reflected:"
      ],
      "metadata": {
        "id": "cust_state_md_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot = graph.get_state(config)\n",
        "\n",
        "print({k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")})"
      ],
      "metadata": {
        "id": "cust_state_code_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5b7c16-033a-4ddc-a89f-1854f61cf446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'LangGraph (library)', 'birthday': 'Jan 17, 2024'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5P3pQ0y-e6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual state updates will generate a trace in LangSmith. If desired, they can also be used to control human-in-the-loop workflows. Use of the `interrupt` function is generally recommended instead, as it allows data to be transmitted in a human-in-the-loop interaction independently of state updates.\n",
        "\n",
        "## Congratulations! ‚ú®\n",
        "\n",
        "You've added custom keys to the state to facilitate a more complex workflow and learned how to generate state updates from inside tools.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "There's one more concept to review before finishing the LangGraph basics tutorials: connecting checkpointing and state updates to **time travel**."
      ],
      "metadata": {
        "id": "cust_state_md_9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-IfVcbUmWLIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time travel ‚è≥\n",
        "\n",
        "In a typical chatbot workflow, the user interacts with the bot one or more times to accomplish a task. Memory and human-in-the-loop enable checkpoints in the graph state and control future responses.\n",
        "\n",
        "What if you want a user to be able to start from a previous response and explore a different outcome? Or what if you want users to be able to rewind your chatbot's work to fix mistakes or try a different strategy, something that is common in applications like autonomous software engineers?\n",
        "\n",
        "You can create these types of experiences using LangGraph's built-in **time travel** functionality.\n",
        "\n",
        "**Note**: This tutorial builds on *Customize state*, but for simplicity, we will use the tool-using graph with memory for demonstration."
      ],
      "metadata": {
        "id": "time_travel_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup the Graph\n",
        "\n",
        "First, let's set up the graph we'll use for time travel. This will be a tool-using agent with memory."
      ],
      "metadata": {
        "id": "setup_graph_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "setup_graph_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Add steps\n",
        "\n",
        "Add steps to your graph by interacting with it. Every step will be checkpointed in its state history:"
      ],
      "metadata": {
        "id": "add_steps_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"I'm learning LangGraph. \"\n",
        "                    \"Could you do some research on it for me?\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "add_steps_code_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f9c51d-272a-49f2-860d-b0c02a746eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I'm learning LangGraph. Could you do some research on it for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (1941d1cc-5e09-4f53-8362-735ec81c077c)\n",
            " Call ID: 1941d1cc-5e09-4f53-8362-735ec81c077c\n",
            "  Args:\n",
            "    query: LangGraph\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"What is LangGraph? - IBM\", \"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent‚Äôs state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.942002, \"raw_content\": null}, {\"title\": \"langchain-ai/langgraph: Build resilient language agents as graphs. - GitHub\", \"url\": \"https://github.com/langchain-ai/langgraph\", \"content\": \"GitHub - langchain-ai/langgraph: Build resilient language agents as graphs. langchain-ai.github.io/langgraph/ Trusted by companies shaping the future of agents ‚Äì including Klarna, Replit, Elastic, and more ‚Äì LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials. LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. LangGraph Platform ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. langchain-ai.github.io/langgraph/\", \"score\": 0.902687, \"raw_content\": null}], \"response_time\": 1.15}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph, created by LangChain, is an open-source AI agent framework designed for building, deploying, and managing complex generative AI agent workflows.\n",
            "\n",
            "At its core, LangGraph uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. In LangGraph, \"nodes\" represent individual components or agents within an AI workflow. It provides transparency into the agent's state and processes, allowing for enhanced decision-making by enabling AI agents to analyze past actions and feedback.\n",
            "\n",
            "LangGraph is particularly useful for developing AI solutions and workflows that require long-running and stateful agents, such as chatbots and state graphs. While it can be used standalone, it also integrates seamlessly with other LangChain products.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    \"Ya that's helpful. Maybe I'll \"\n",
        "                    \"build an autonomous agent with it!\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    config,\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "add_steps_code_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5d1c39-56be-4033-867a-1579cac971b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Ya that's helpful. Maybe I'll build an autonomous agent with it!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (6969dc07-b5ed-4232-9079-836399a957e8)\n",
            " Call ID: 6969dc07-b5ed-4232-9079-836399a957e8\n",
            "  Args:\n",
            "    query: how to build an autonomous agent with LangGraph\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"how to build an autonomous agent with LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"How to Build the Ultimate AI Automation with Multi-Agent Collaboration\", \"url\": \"https://blog.langchain.dev/how-to-build-the-ultimate-ai-automation-with-multi-agent-collaboration/\", \"content\": \"Editor's Note: The following post is authored by Assaf Elovic, Head of R&D at¬†Wix. In this blog, he walks through how to build an autonomous research assistant using LangGraph with a team of specialized agents. In this article you will learn why multi agent workflows are the current best standard and how to build the optimal autonomous research multi-agent assistant using¬†LangGraph. The Research Agent Team As seen below, the automation process is based on the following stages: Planning the research, data collection and analysis, review and revision, writing the report and finally publication: workflow.add_node(\\\"browser\\\", research_agent.run_initial_research) workflow.add_node(\\\"planner\\\", editor_agent.plan_research) workflow.add_node(\\\"researcher\\\", editor_agent.run_parallel_research) workflow.add_node(\\\"researcher\\\", research_agent.run_depth_research) After finalizing the agents, states and graphs, it‚Äôs time to run our¬†research assistant!\", \"score\": 0.82117355, \"raw_content\": null}, {\"title\": \"How to Build AI Agents with LangGraph: A Step-by-Step Guide\", \"url\": \"https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832\", \"content\": \"In this step, we‚Äôll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the user‚Äôs input and tool output. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). Next, we define the tools that the assistant will use during the interaction, with the primary tool being compute_savings, which calculates potential savings based on the user's monthly electricity cost. By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs.\", \"score\": 0.7940821, \"raw_content\": null}], \"response_time\": 2.27}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Building an autonomous agent with LangGraph often involves creating a multi-agent collaboration system. Here's a general approach based on the information available:\n",
            "\n",
            "1.  **Define the Agent's State:** This involves setting up how the AI agent manages the ongoing context of the conversation or task. You'll need to define how it responds to user input and the output from its tools.\n",
            "\n",
            "2.  **Define Tools:** Specify the tools that your agent will use to perform actions. These tools are essentially functions or modules that the agent can call upon. For example, a research assistant might have tools for browsing the web or analyzing data.\n",
            "\n",
            "3.  **Design the Workflow (Graph):** LangGraph's core strength is its graph-based architecture. You'll define nodes (individual components or agents) and the relationships between them. For an autonomous research assistant, a workflow might include stages like:\n",
            "    *   Planning the research\n",
            "    *   Data collection and analysis\n",
            "    *   Review and revision\n",
            "    *   Writing the report\n",
            "    *   Publication\n",
            "\n",
            "4.  **Implement Agents:** For each stage or specific task, you might implement specialized agents. For instance, a \"browser\" agent, a \"planner\" agent, or a \"researcher\" agent, each with its own responsibilities.\n",
            "\n",
            "5.  **Configure Responses and Triggers:** Configure how the AI agent will respond to user input and how it will trigger different functions or transitions between nodes in the graph.\n",
            "\n",
            "By following these steps, you can create complex, long-running, and stateful AI agents using LangGraph.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9erSUNI3v2D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replay the full state history\n",
        "\n",
        "Now that you have added steps to the chatbot, you can replay the full state history to see everything that occurred. We will identify a specific state to 'travel' back to."
      ],
      "metadata": {
        "id": "replay_history_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_replay = None\n",
        "for state in graph.get_state_history(config):\n",
        "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
        "    print(\"-\" * 80)\n",
        "    if len(state.values[\"messages\"]) == 4:\n",
        "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
        "        to_replay = state"
      ],
      "metadata": {
        "id": "replay_history_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609d63e5-470e-441f-b78c-877f68862bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Messages:  8 Next:  ()\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  7 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  6 Next:  ('tools',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  5 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ('__start__',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  4 Next:  ()\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  3 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  2 Next:  ('tools',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  1 Next:  ('chatbot',)\n",
            "--------------------------------------------------------------------------------\n",
            "Num Messages:  0 Next:  ('__start__',)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoints are saved for every step of the graph and span invocations, so you can rewind across a full thread's history.\n",
        "\n",
        "## 4. Resume from a checkpoint (Time Travel!)\n",
        "\n",
        "Resume from the `to_replay` state. The checkpoint's `to_replay.config` contains a `checkpoint_id`. Providing this value tells LangGraph's checkpointer to load the state from that moment in time. We'll start *before* the second tool call and can now proceed differently."
      ],
      "metadata": {
        "id": "resume_checkpoint_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(to_replay.next)\n",
        "print(to_replay.config)"
      ],
      "metadata": {
        "id": "resume_checkpoint_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473285dc-1cd7-4933-aab8-07315305d157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "()\n",
            "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03b27c-3f3d-65f4-8003-a61f61509745'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
        "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
        "    if \"messages\" in event:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3f6KWN-wDWQ",
        "outputId": "c4c99324-0d68-497d-8a47-e6a178d746b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangGraph, created by LangChain, is an open-source AI agent framework designed for building, deploying, and managing complex generative AI agent workflows.\n",
            "\n",
            "At its core, LangGraph uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. In LangGraph, \"nodes\" represent individual components or agents within an AI workflow. It provides transparency into the agent's state and processes, allowing for enhanced decision-making by enabling AI agents to analyze past actions and feedback.\n",
            "\n",
            "LangGraph is particularly useful for developing AI solutions and workflows that require long-running and stateful agents, such as chatbots and state graphs. While it can be used standalone, it also integrates seamlessly with other LangChain products.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph loaded the state from the past but proceeded with the *new* user input, effectively changing the conversation's direction. You've successfully traveled through time!\n",
        "\n",
        "## Congratulations! üöÄ\n",
        "\n",
        "You've now used time-travel checkpoint traversal in LangGraph. Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications.\n",
        "\n",
        "## Learn more\n",
        "\n",
        "Take your LangGraph journey further by exploring deployment and advanced features:\n",
        "\n",
        "* [LangGraph Server quickstart](https://langchain-ai.github.io/langgraph/tutorials/quickstarts/quickstart_server/): Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\n",
        "* [LangGraph Platform quickstart](https://langchain-ai.github.io/langgraph/tutorials/quickstarts/quickstart_platform/): Deploy your LangGraph app using LangGraph Platform.\n",
        "* [LangGraph Platform concepts](https://langchain-ai.github.io/langgraph/concepts/platform/): Understand the foundational concepts of the LangGraph Platform."
      ],
      "metadata": {
        "id": "congrats_learn_more_time_travel"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}